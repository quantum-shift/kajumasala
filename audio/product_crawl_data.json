[
,
,
,

],

],

],
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=elevenlabs.io",
    "text": "Instant Docs and SDKs for your API\nElevate your developer experience\nSDKs that are designed by language experts\nJust run \nImport your API spec\nSelect languages to generate\nAdd custom code\nPublish packages\nEvery feature you need, built-in\nAutomatic updates via CI/CD\nOAuth 2.0\nServer sent events\nAuto pagination\nPolymorphism (Unions)\nAudiences\nMultipart form uploads\nRetries with exponential backoff\nIdempotency headers\nCode Snippets over API\nDependency vulnerability scanning\nEverything you need to impress your users\nDocs as code\nSEO optimized\nPreview deployments\nAPI key injection\nAccess control (RBAC)\nFederated authentication\nVersioning\nWebSockets\nComponent library\nCustomize with JavaScript and CSS\nBring your own React component\nAI chat\n/llms.txt\nYour SDK team.\nYour Docs team.\nPublish client libraries in popular languages.\nAPI documentation tailored to your brand.\nStart with OpenAPI. \n SDKs in multiple languages and interactive API documentation tailored to your brand.\nStart with Docs, SDKs, or both — it’s your choice.\nEvery generated SDK respects the idioms of the language, contains strong types, inlined docs, and intuitive error handling.\nCheck out live SDKs built with Fern\nCheck out live SDKs built with Fern\nWe evaluated several SDK generators and Fern stood out for its clean, language-native, and thoughtfully architected code. \n‍\nJohn Fellman\nHead of Engineering, Developer Platform\nFern generates and publishes client libraries, so you can focus on building the API.\nStart with OpenAPI, AsyncAPI, gRPC or our simpler format, the Fern Definition.\nWe support TypeScript, Python, Go, Java, Ruby, C# & PHP. \nSwift & Rust coming soon.\nExtend the generated client with utility functions and custom logic. \nYour changes won’t be overwritten on re-generation.\nFern semantically versions and publishes packages to each registry (e.g. npm, pypi, maven)\nFern SDKs support OAuth 2.0, server-sent events, auto-pagination and more.\nUse the Fern CLI to generate SDKs as part of your release process.\nHandle OAuth 2.0 authentication seamlessly with automatic token management and refresh.\nStream real-time updates effortlessly with built-in SSE support.\nAutomatically iterate through paginated data with built-in pagination helpers.\nHandle complex polymorphic data with native type safety in every language.\nFilter your API definition to the endpoints and properties you want included in your SDK.\nSeamlessly handle multipart uploads with built-in utilities for streaming binary data and MIME types.\nAutomatically retry failed requests and maximize the uptime of your API.\nSend idempotency headers to ensure safe retries for sensitive operations (e.g. sending payments).\nFetch SDK code samples from our API and embed them in your application.\nFern adheres to an SLA to detect and remediate vulnerable dependencies.\nStunning by default, easy to update, and designed to convert.\nUse git to version and release changes to your documentation.\nFern uses industry best practices to make your website fast and SEO optimized to rank.\nPreview changes to your documentation with a unique URL for each pull request.\nAutomatically populate API keys so that users can instantly make API calls and see realistic responses.\nAssign role-based permissions and configure what content a user can access.\nSecure your documentation behind auth. Fern supports SSO, OAuth, and username + password.\nDocument and manage multiple versions of your product or API to support users across different releases.\nDocument your WebSocket send and receive events with an AsyncAPI spec. \nInteract with the WebSocket via an API Explorer that lets you establish live connections.\nLeverage prebuilt components like cards, accordions, tabs, and code blocks.\nCustomize your docs with JavaScript and CSS to match your brand identity and extend functionality.\nBuild custom React components and embed them in your docs.\nInteract with an AI assistant trained on your docs to quickly find answers and troubleshoot issues.\nServe an llms.txt with no extra effort, allowing agents to understand the capabilities of your product.\nWe partner with you to perfect your OpenAPI spec and launch SDKs that scale to millions of downloads.\nPowering SDKs for world-class companies\nOur team creates a custom theme, migrates all your content, and launches your site.\nProviding gorgeous Docs for world-class companies\nBuilt with\nFern automated SDK maintenance, allowing us to support all popular programming languages without needing dedicated engineering resources. \n-- they bring expert-level guidance in OpenAPI and SDK design.\nLiz Moy\nSenior Developer Relations Engineer\nPartnering with Fern, \n Fern also generates our documentation website that perfectly matches Cohere's unique brand identity.\nBilly Trend\nDeveloper Relations Engineer\nWe used an alternative SDK generator, OpenAPI Generator, for years, and customers constantly complained. \nGil Feig\nCTO\nThey embedded with our docs team, migrated over 700+ pages of content, and created an intuitive information architecture across API reference, guides and SDK reference.\nMatt Makai\nVP of Developer Experience\nCompared to OpenAPI, it's more ergonomic and comes with powerful capabilities - you can organize endpoints by resource, import types across files, and represent complex APIs like server sent events and websockets.\nKabir Goel\nFounding Engineer\nKeeping our docs, SDKs, and server implementation in sync was a nightmare. Fern enabled us to adopt API-first development — our engineers update the spec, run fern generate, and everything stays perfectly aligned.\nSteve Yazicioglu\nHead of Engineering\nFern has been \n. We often get compliments in sales calls and during new pilot onboardings about the quality of our docs.\nMichael Bauer\nSoftware Engineer at Nominal\nDelight your developers with SDKs and Docs."
}
,
{
    "url": "https://elevenlabs.io/docs/resources/zero-retention-mode",
    "text": "Background\nWhat is zero retention mode?\nWho has access to zero retention mode?\nWhen can a customer use zero retention mode?\nHow does zero retention mode work?\nWhat products are configured for zero retention mode?\nWhat are some limitations of zero retention mode?\nHow retention works if zero retention mode is not active?\nData backup (When zero retention mode is not used)\nAccount deletion (When zero retention mode is not used)\nZero retention mode (Enterprise)\nLearn how to use Zero Retention Mode to protect sensitive data.\nBy default, we retain data, in accordance with our Privacy Policy, to enhance our services, troubleshoot issues, and ensure the security of our systems. However, for some enterprise customers, we offer a “Zero Retention Mode” option for specific products. In this Zero Retention Mode, most data in requests and responses are immediately deleted once the request is completed.\nZero Retention Mode provides an additional level of security and peace of mind for especially sensitive workflows. When enabled, logging of certain data points is restricted, including:\nThis data is related to the processing of the request, and can only be seen by the user doing the request and the volatile memory of the process serving the request. None of this data is sent at any point to a database where data is stored long term.\nEnterprise customers can use zero Retention Mode.\nIt is primarily intended for use by our customers in the healthcare and banking sector, and other customers who may use our services to process sensitive information.\nZero Retention Mode is available to select enterprise customers. However, access to this feature may be restricted if ElevenLabs determines a customer’s use case to be high risk, if an account is flagged by an automated system for additional moderation or at ElevenLabs’ sole discretion. In such cases, the enterprise administrator will be promptly notified of the restriction.\nZero Retention Mode only works for API requests, specifically:\nAfter setup, check the request history to verify zero Retention Mode is enabled. If enabled, there should be no requests in the history.\nZero retention mode can be used by sending \n with the product which supports it.\nFor example, in Text to Speech API, you can set the query parameter \n to False like this:\nTroubleshooting and support for zero Retention Mode is limited. Because of the configuration, we will not be able to diagnose issues with TTS/STS generations. Debugging will be more difficult as a result.\nCustomers by default have history preservation enabled. All customers can use the API to delete generations at any time. This action will immediately remove the corresponding audio and text from our database; however, debugging and moderation logs may still retain data related to the generation.\nFor any retained data, we regularly back up such data to prevent data loss in the event of any unexpected incidents. Following data deletion, database items are retained in backups for up to 30 days After this period, the data expires and is not recoverable.\nAll data is deleted from our systems permanently when you delete your account. This includes all data associated with your account, such as API keys, request history, and any other data stored in your account. We also take commercially reasonable efforts to delete debugging data related to your account."
}
,
{
    "url": "https://elevenlabs.io/docs/resources/error-messages",
    "text": "Dashboard errors\nAPI errors\nCode 400/401\nCode 403\nCode 429\nError messages\nExplore error messages and solutions.\nThis guide includes an overview of error messages you might see in the ElevenLabs dashboard & API.\nIf error messages persist after following these solutions, please \n for further\nassistance.\nIf error messages persist after following these solutions, please \n for further\nassistance."
}
,
{
    "url": "https://elevenlabs.io/docs/best-practices/prompting",
    "text": "Pauses\nPronunciation\nPhoneme Tags\nAlias Tags\nPronunciation Dictionaries\nPronunciation Dictionary Example\nEmotion\nPace\nTips\nCreative control\nNarrative styling\nLayered outputs\nPhonetic experimentation\nManual adjustments\nFeedback iteration\nPrompting\nLearn how to control delivery, pronunciation & emotion of text to speech.\nWe are actively working on \n to give you even greater control over outputs.\nThis guide provides techniques to enhance text-to-speech outputs using ElevenLabs models. Experiment with these methods to discover what works best for your needs. These techniques provide a practical way to achieve nuanced results until advanced features like \n are rolled out.\nUse \n for natural pauses up to 3 seconds.\nUsing too many break tags in a single generation can cause instability. The AI might speed up, or\nintroduce additional noises or audio artifacts. We are working on resolving this.\nAlternatives to \n include dashes (- or —) for short pauses or ellipses (…) for hesitant tones. However, these are less consistent.\nSpecify pronunciation using \n. Supported alphabets include \n Arpabet and the \n.\nPhoneme tags are only compatible with “Eleven English v1” and “Eleven Turbo v2”\n\n.\nWe recommend using CMU Arpabet for consistent and predictable results with current AI models. While IPA can be effective, CMU Arpabet generally offers more reliable performance.\nPhoneme tags only work for individual words. If for example you have a name with a first and last name that you want to be pronounced a certain way, you will need to create a phoneme tag for each word.\nEnsure correct stress marking for multi-syllable words to maintain accurate pronunciation. For example:\nFor models that don’t support phoneme tags, you can try writing words more phonetically. You can also employ various tricks such as capital letters, dashes, apostrophes, or even single quotation marks around a single letter or letters.\nAs an example, a word like “trapezii” could be spelt “trapezIi” to put more emphasis on the “ii” of the word.\nYou can either replace the word directly in your text, or if you want to specify pronunciation using other words or phrases when using a pronunciation dictionary, you can use alias tags for this. This can be useful if you’re generating using Multilingual v2 or Turbo v2.5, which don’t support phoneme tags. You can use pronunciation dictionaries with Studio, Dubbing Studio and Speech Synthesis via the API.\nFor example, if your text includes a name that has an unusual pronunciation that the AI might struggle with, you could use an alias tag to specify how you would like it to be pronounced:\nIf you want to make sure that an acronym is always delivered in a certain way whenever it is incountered in your text, you can use an alias tag to specify this:\nSome of our tools, such as Studio and Dubbing Studio, allow you to create and upload a pronunciation dictionary. These allow you to specify the pronunciation of certain words, such as character or brand names, or to specify how acronyms should be read.\nPronunciation dictionaries allow this functionality by enabling you to upload a lexicon or dictionary file that specifies pairs of words and how they should be pronounced, either using a phonetic alphabet or word substitutions.\nWhenever one of these words is encountered in a project, the AI model will pronounce the word using the specified replacement.\nTo provide a pronunciation dictionary file, open the settings for a project and upload a file in either TXT or the \n. When a dictionary is added to a project it will automatically recalculate which pieces of the project will need to be re-converted using the new dictionary file and mark these as unconverted.\nCurrently we only support pronunciation dictionaries that specify replacements using phoneme or alias tags.\nBoth phonemes and aliases are sets of rules that specify a word or phrase they are looking for, referred to as a grapheme, and what it will be replaced with. Please note that searches are case sensitive. When checking for a replacement word in a pronunciation dictionary, the dictionary is checked from start to end and only the very first replacement is used.\nHere is an example pronunciation dictionary that specifies in IPA the pronunciation of “Apple” with IPA of “ˈæpl̩” and “UN” with an alias of “United Nations”:\nTo generate a pronunciation dictionary \n file, there are a few open source tools available:\nConvey emotions through narrative context or explicit dialogue tags. This approach helps the AI understand the tone and emotion to emulate.\nExplicit dialogue tags yield more predictable results than relying solely on context, however the model will still speak out the emotional delivery guides. These can be removed in post-production using an audio editor if unwanted.\nPacing can be controlled by writing in a natural, narrative style. For voice cloning, longer, continuous samples are recommended to avoid pacing issues like unnaturally fast speech.\nSample Length: Use longer, continuous samples for voice cloning to avoid pacing issues.\nNarrative Style: Write in a narrative style to naturally control pacing and emotion, similar to scriptwriting.\nInconsistent pauses: Ensure \n syntax is used for\npauses.\nEmotion mismatch: Add narrative context or explicit tags to guide emotion.\n \n\n\nExperiment with alternative phrasing to achieve desired pacing or emotion. For complex sound\neffects, break prompts into smaller, sequential elements and combine results manually.\nWhile we are actively developing a “Director’s Mode” to give users even greater control over outputs, here are some interim techniques to maximize creativity and precision:\nWrite prompts in a narrative style, similar to scriptwriting, to guide tone and pacing effectively.\nGenerate sound effects or speech in segments and layer them together using audio editing software for more complex compositions.\nIf pronunciation isn’t perfect, experiment with alternate spellings or phonetic approximations to achieve desired results.\nCombine individual sound effects manually in post-production for sequences that require precise timing.\nIterate on results by tweaking descriptions, tags, or emotional cues."
}
,
{
    "url": "https://buildwithfern.com/terms-of-service",
    "text": "Terms of Service\nInformation We Collect\nLog Data\nDevice Data\nPersonal Information\nLegitimate Reasons for Processing Your Personal Information\nCollection and Use of Information\nSecurity of Your Personal Information\nHow Long We Keep Your Personal Information\nChildren’s Privacy\nInternational Transfers of Personal Information\nYour Rights and Controlling Your Personal Information\nLimits of Our Policy\nBranding and Fair Use\nContact Us\nTable of contents\nPublish client libraries in popular languages.\nAPI documentation tailored to your brand.\nWelcome to the Birch Solutions, Inc. dba Fern (“\n”, “\n” “\n” or “\n”) website. Please read these Terms of Service (the “\n”) carefully because they govern your use of our websites including buildwithfern.com and within our \n.\nThis Agreement is between Fern and the company or person accessing or using the Product. This Agreement consists of: (1) the Order Form and (2) the Key Terms, both of which are on the Cover Page below, and (3) the Common Paper \n (“Standard Terms”). Any modifications to the Standard Terms made in the Cover Page will control over conflicts with the Standard Terms. Capitalized words have the meanings or descriptions given in the Cover Page or the Standard Terms.\nIf you are accessing or using the Product on behalf of your company, you represent that you are authorized to accept this Agreement on behalf of your company. By signing up, accessing, or using the Product, Customer indicates its acceptance of this Agreement and agrees to be bound by the terms and conditions of this Agreement.\nCover Page\n Fern offers cloud-based software to define APIs and generate code, including SDKs and documentation.\n The Effective Date\n 12 month(s)\n At least 30 days before the end of the current Subscription Period.\nSection 5.2 of the Standard Terms is replaced with: Certain parts of the Product have different pricing plans, which are available at Provider’s pricing page (\n). Within the Payment Period, Customer will pay Provider fees based on the Product tier selected at the time of account creation and Customer’s usage per Subscription Period. Provider may update Product pricing by giving at least 30 days notice to Customer (including by email or notification within the Product), and the change will apply in the next Subscription Period.\n 30 day(s) from Customer's receipt of invoice\n  Annually\n The company or person who accesses or uses the Product. If the person accepting this Agreement is doing so on behalf of a company, all use of the word \"Customer\" in the Agreement will mean that company.\n Birch Solutions, Inc. dba Fern\n The date Customer first accepts this Agreement.\n Any action, proceeding, or claim that the Cloud Service, when used by Customer according to the terms of the Agreement, violates, misappropriates, or otherwise infringes upon anyone else’s intellectual property or other proprietary rights.\n Any action, proceeding, or claim that (1) the Customer Content, when used according to the terms of the Agreement, violates, misappropriates, or otherwise infringes upon anyone else’s intellectual property or other proprietary rights; or (2) results from Customer’s breach or alleged breach of Section 2.1 (Restrictions on Customer).\nThe fees paid or payable by Customer to provider in the 12 month period immediately before the claim\n  The laws of the State of Delaware\n The state or federal courts located in Delaware\nFor Provider: \nFor Customer: The main email address on Customer's account\n Modifying Section 14.7 of the Standard Terms, Provider may identify Customer and use Customer’s logo and trademarks on Provider’s website and in marketing materials to identify Customer as a user of the Product. Customer hereby grants Provider a non-exclusive, royalty-free license to do so in connection with any marketing, promotion, or advertising of Provider or the Product during the length of the Agreement.\nInformation we collect includes both information you knowingly and actively provide us when using or participating in any of our services and promotions, and any information automatically sent by your devices in the course of accessing our products and services.\nWhen you visit our website, our servers may automatically log the standard data provided by your web browser. It may include your device’s Internet Protocol (IP) address, your browser type and version, the pages you visit, the time and date of your visit, the time spent on each page, and other details about your visit.\nAdditionally, if you encounter certain errors while using the site, we may automatically collect data about the error and the circumstances surrounding its occurrence. This data may include technical details about your device, what you were trying to do when the error happened, and other technical information relating to the problem. You may or may not receive notice of such errors, even in the moment they occur, that they have occurred, or what the nature of the error is.\nPlease be aware that while this information may not be personally identifying by itself, it may be possible to combine it with other data to personally identify individual persons.\nWhen you visit our website or interact with our services, we may automatically collect data about your device, such as:\nData we collect can depend on the individual settings of your device and software. We recommend checking the policies of your device manufacturer or software provider to learn what information they make available to us.\nWe may ask for personal information which may include one or more of the following:\nWe only collect and use your personal information when we have a legitimate reason for doing so. In which instance, we only collect personal information that is reasonably necessary to provide our services to you.\nWe may collect personal information from you when you do any of the following on our website:\nWe may collect, hold, use, and disclose information for the following purposes, and personal information will not be further processed in a manner that is incompatible with these purposes:\nPlease be aware that we may combine information we collect about you with general information or research data we receive from other trusted sources.\nWhen we collect and process personal information, and while we retain this information, we will protect it within commercially acceptable means to prevent loss and theft, as well as unauthorized access, disclosure, copying, use, or modification.\nAlthough we will do our best to protect the personal information you provide to us, we advise that no method of electronic transmission or storage is 100% secure, and no one can guarantee absolute data security. We will comply with laws applicable to us in respect of any data breach.\nYou are responsible for selecting any password and its overall security strength, ensuring the security of your own information within the bounds of our services.\nWe keep your personal information only for as long as we need to. This time period may depend on what we are using your information for, in accordance with this privacy policy. If your personal information is no longer required, we will delete it or make it anonymous by removing all details that identify you.\nHowever, if necessary, we may retain your personal information for our compliance with a legal, accounting, or reporting obligation or for archiving purposes in the public interest, scientific, or historical research purposes or statistical purposes.\nWe do not aim any of our products or services directly at children under the age of 13, and we do not knowingly collect personal information about children under 13.\nDisclosure of Personal Information to Third Parties We may disclose personal information to:\nThe personal information we collect is stored and/or processed in United States, or where we or our partners, affiliates, and third-party providers maintain facilities.\nThe countries to which we store, process, or transfer your personal information may not have the same data protection laws as the country in which you initially provided the information. If we transfer your personal information to third parties in other countries: (i) we will perform those transfers in accordance with the requirements of applicable law; and (ii) we will protect the transferred personal information in accordance with this privacy policy.\nYou always retain the right to withhold personal information from us, with the understanding that your experience of our website may be affected. We will not discriminate against you for exercising any of your rights over your personal information. If you do provide us with personal information you understand that we will collect, hold, use and disclose it in accordance with this privacy policy. You retain the right to request details of any personal information we hold about you.\nIf we receive personal information about you from a third party, we will protect it as set out in this privacy policy. If you are a third party providing personal information about somebody else, you represent and warrant that you have such person’s consent to provide the personal information to us.\nIf you have previously agreed to us using your personal information for direct marketing purposes, you may change your mind at any time. We will provide you with the ability to unsubscribe from our email-database or opt out of communications. Please be aware we may need to request specific information from you to help us confirm your identity.\nIf you believe that any information we hold about you is inaccurate, out of date, incomplete, irrelevant, or misleading, please contact us using the details provided in this privacy policy. We will take reasonable steps to correct any information found to be inaccurate, incomplete, misleading, or out of date.\nIf you believe that we have breached a relevant data protection law and wish to make a complaint, please contact us using the details below and provide us with full details of the alleged breach. We will promptly investigate your complaint and respond to you, in writing, setting out the outcome of our investigation and the steps we will take to deal with your complaint. You also have the right to contact a regulatory body or data protection authority in relation to your complaint.\nAt our discretion, we may change our privacy policy to reflect updates to our business processes, current acceptable practices, or legislative or regulatory changes. If we decide to change this privacy policy, we will post the changes here at the same link by which you are accessing this privacy policy.\nIf the changes are significant, or if required by applicable law, we will contact you (based on your selected preferences for communications from us) and all our registered users with the new details and links to the updated or changed policy.\nIf required by law, we will get your permission or give you the opportunity to opt in to or opt out of, as applicable, any new uses of your personal information.\nIt's not possible for a Customer to remove the small \"Build with Fern\" link that appears in published documentation through tactics, including but not limited to, custom CSS or global JavaScript.\nFor any questions or concerns regarding your privacy, you may contact us using the following details:\nSupport \n‍\nDelight your developers with SDKs and Docs."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/voices",
    "text": "Overview\nVoice categories\nManaging voices\nSupported languages\nFAQ\nVoices\nLearn how to create, customize, and manage voices with ElevenLabs.\nElevenLabs provides models for voice creation & customization. The platform supports a wide range of voice options, including voices from our extensive \n, voice cloning, and artificially designed voices using text prompts.\nThe \n contains over 5,000 voices shared by the ElevenLabs community. Use it to:\nShare your voice with the community, set your terms, and earn cash rewards when others use it.\nWe’ve paid out over \n already.\nLearn how to use voices from the voice library\nClone your own voice from 30-second samples with Instant Voice Cloning, or create hyper-realistic voices using Professional Voice Cloning.\nVoice-captcha technology is used to verify that \n voice clones are created from your own voice samples.\nLearn how to create instant & professional voice clones\nWith \n, you can create entirely new voices by specifying attributes like age, gender, accent, and tone. Generated voices are ideal for:\nThe voice design tool creates 3 voice previews, simply provide:\nIntegrate voice design into your application.\nLearn how to craft voices from a single prompt.\nOur curated set of default voices is optimized for core use cases. These voices are:\nDefault voices are available to all users via the \n tab in the \n. Default voices were previously referred to as\n\n voices. The latter term is still used when accessing default voices via the API.\nAll voices can be managed through \n, where you can:\nLearn how to manage your voice collection in \n.\n: Try searching by specific accents or genres, such as “Australian narration” or “child-like character.”\nAll ElevenLabs voices support multiple languages. Experiment by converting phrases like \n into speech to hear how your own voice sounds across different languages.\nElevenLabs supports voice creation in 32 languages. Match your voice selection to your target region for the most natural results.\nOur v2 models support 29 languages:\nFlash v2.5 supports 32 languages - all languages from v2 models plus:\nYes, you can create custom voices with Voice Design or clone voices using Instant or\nProfessional Voice Cloning. Both options are accessible in \n.\nInstant Voice Cloning uses short audio samples for near-instantaneous voice creation.\nProfessional Voice Cloning requires longer samples but delivers hyper-realistic, high-quality\nresults.\nProfessional Voice Clones can be shared privately or publicly in the Voice Library. Generated\nvoices and Instant Voice Clones cannot currently be shared.\nUse \n to search, filter, and organize your voice collection. You can also delete,\ntag, and categorize voices for easier management.\nUse clean and consistent audio samples. For Professional Voice Cloning, provide a variety of\nrecordings in the desired speaking style.\nYes, Professional Voice Clones can be shared in the Voice Library. Instant Voice Clones and\nGenerated Voices cannot currently be shared.\nGenerated Voices are ideal for unique characters in games, animations, and creative\nstorytelling.\nGo to \n in your dashboard or access it via API."
}
,
{
    "url": "https://elevenlabs.io/docs/resources/libraries",
    "text": "Official REST API libraries\nConversational AI libraries\nLibraries & SDKs\nExplore language-specific libraries for using the ElevenLabs API.\nElevenLabs provides officially supported libraries that are updated with the latest features available in the \n.\nTest and explore all ElevenLabs API endpoints using our official \n.\nThese libraries are designed for use with ElevenLabs \n."
}
,
{
    "url": "https://elevenlabs.io/docs/best-practices/latency-optimization",
    "text": "Use Flash models\nLeverage streaming\nStreaming\nWebsockets\nConsider geographic proximity\nChoose appropriate voices\nLatency optimization\nLearn how to optimize text-to-speech latency.\nThis guide covers the core principles for improving text-to-speech latency.\nWhile there are many individual techniques, we’ll group them into \n.\nEnterprise customers benefit from increased concurrency limits and priority access to our rendering queue. \n to learn more about our enterprise\nplans.\n deliver ~75ms inference speeds, making them ideal for real-time applications. The trade-off is a slight reduction in audio quality compared to \n.\n75ms refers to model inference time only. Actual end-to-end latency will vary with factors such as\nyour location & endpoint type used.\nThere are three types of text-to-speech endpoints available in our \n:\nStreaming endpoints progressively return audio as it is being generated in real-time, reducing the time-to-first-byte. This endpoint is recommended for cases where the input text is available up-front.\nStreaming is supported for the \n API, \n API & \n API.\nThe \n supports bidirectional streaming making it perfect for applications with real-time text input (e.g. LLM outputs).\nSetting \n to true automatically handles generation triggers, removing the need to\nmanually manage chunk strategies.\nIf \n is disabled, the model will wait for enough text to match the chunk schedule before starting to generate audio.\nFor instance, if you set a chunk schedule of 125 characters but only 50 arrive, the model stalls until additional characters come in—potentially increasing latency.\nFor implementation details, see the \n.\nBecause our models are served in the US, your geographic location will affect the network latency you experience.\nFor example, using Flash models with Websockets, you can expect the following TTFB latencies:\nWe are actively working on deploying our models in EU and Asia. These deployments will bring\nspeeds closer to those experienced by US customers.\nWe have observed that in some cases, voice selection can impact latency. Here’s the order from fastest to slowest:\nHigher audio quality output formats can increase latency. Be sure to balance your latency requirements with audio fidelity needs."
}
,
{
    "url": "https://elevenlabs.io/docs/resources/troubleshooting",
    "text": "General\nStudio (formerly Projects)\nTroubleshooting\nExplore common issues and solutions.\nOur models are non-deterministic, meaning outputs can vary based on inputs. While we strive to enhance predictability, some variability is inherent. This guide outlines common issues and preventive measures.\nIf the generated voice output varies in volume or tone, it is often due to inconsistencies in the voice clone training audio.\nTo minimize issues, consider breaking your text into smaller segments. This approach helps maintain consistent volume and reduces degradation over longer audio generations. Utilize our Studio feature to generate several smaller audio segments simultaneously, ensuring better quality and consistency.\nRefer to our guides for optimizing Instant and Professional Voice Clones for best practices and\nadvice.\nThe multilingual models may rarely mispronounce certain words, even in English. This issue appears to be somewhat arbitrary but seems to be voice and text-dependent. It occurs more frequently with certain voices and text, especially when using words that also appear in other languages.\nThe AI can sometimes switch languages or accents throughout a single generation, especially if that generation is longer in length. This issue is similar to the mispronunciation problem and is something we are actively working to improve.\nThe models may mispronounce certain numbers, symbols and acronyms. For example, the numbers “1, 2, 3” might be pronounced as “one,” “two,” “three” in English. To ensure correct pronunciation in another language, write them out phonetically or in words as you want them to be spoken.\nCorrupt speech is a rare issue where the model generates muffled or distorted audio. This occurs\nunpredictably, and we have not identified a cause. If encountered, regenerate the section to\nresolve the issue.\nAudio quality may degrade during extended text-to-speech conversions, especially with the Multilingual v1 model. To mitigate this, break text into sections under 800 characters.\nFor some voices, this voice setting can lead to instability, including inconsistent speed,\nmispronunciation and the addition of extra sounds. We recommend keeping this setting at 0,\nespecially if you find you are experiencing these issues in your generated audio.\nThe import function attempts to import the file you provide to the website. Given the variability in website structures and book formatting, including images, always verify the import for accuracy.\nOccasionally, glitches or sharp breaths may occur between paragraphs. This is rare and differs\nfrom standard Text to Speech issues. If encountered, regenerate the preceding paragraph, as the\nproblem often originates there.\nIf an issue persists after following this troubleshooting guide, please \n."
}
,
{
    "url": "https://elevenlabs.io/docs/product-guides/overview",
    "text": "Product guides\nAdministration\nTroubleshooting\nOverview\nStep by step worflow guides.\nThis section covers everything from account creation to advanced voice cloning, speech synthesis techniques, dubbing, and expert voiceover.\nDiscover how to create speech from text with text to speech\nDiscover how to transform your voice with voice changer\nDiscover how to create cinematic sound effects from text\nManage long-form content with projects\nDiscover how to dub your videos in multiple languages\nDiscover how to create conversational AI agents\nDiscover how to create instant & professional voice clones\nDiscover our voice library with over 5,000 community voices\nDiscover how to craft voices from a single prompt\nDiscover how to get paid when your voice is used\nDiscover how to get paid when your voice is used\nManage long-form audio generation with voiceover studio\nIsolate voices from background noise\nClassify AI-generated speech\nLearn how to manage your account settings\nLearn how to manage your billing information\nLearn how to manage your enterprise workspaces\nLearn how to enable single sign-on for your enterprise"
}
,
{
    "url": "https://buildwithfern.com/privacy-policy",
    "text": "Privacy policy\nInformation We Collect\nLog Data\nDevice Data\nPersonal Information\nLegitimate Reasons for Processing Your Personal Information\nCollection and Use of Information\nSecurity of Your Personal Information\nHow Long We Keep Your Personal Information\nChildren\nInternational Transfers of Personal Information\nYour Rights and Controlling Your Personal Information\nLimits of Our Policy\nChanges to This Privacy Policy\nContact Us\nTable of contents\nPublish client libraries in popular languages.\nAPI documentation tailored to your brand.\nYour privacy is important to us. It is the policy of Birch Solutions, Inc. dba Fern (\"Fern\") to respect your privacy and comply with any applicable law and regulation regarding any personal information we may collect about you, including across our website, \n, and other sites we own and operate.\nPersonal information is any information about you which can be used to identify you. This includes information about you as a person (such as name, address, and date of birth), your devices, payment details, and even information about how you use a website or online service.\nIn the event our site contains links to third-party sites and services, please be aware that those sites and services have their own privacy policies. After following a link to any third-party content, you should read their posted privacy policy information about how they collect and use personal information. This Privacy Policy does not apply to any of your activities after you leave our site.\nThis policy is effective as of August 17, 2022.\nLast updated on July 15, 2025.\nInformation we collect includes both information you knowingly and actively provide us when using or participating in any of our services and promotions, and any information automatically sent by your devices in the course of accessing our products and services. \nWhen you visit or log in to our website, cookies and similar technologies may be used by our online data partners or vendors to associate these activities with other personal information they or others have about you, including by association with your email or home address. We (or service providers on our behalf) may then send communications and marketing to these email or home addresses. You may opt out of receiving this advertising by visiting \nWhen you visit our website, our servers may automatically log the standard data provided by your web browser. It may include your device’s Internet Protocol (IP) address, your browser type and version, the pages you visit, the time and date of your visit, the time spent on each page, and other details about your visit. \nAdditionally, if you encounter certain errors while using the site, we may automatically collect data about the error and the circumstances surrounding its occurrence. This data may include technical details about your device, what you tried to do when the error happened, and other technical information relating to the problem. You may or may not receive notice of such errors, even at the moment they occur, that they have occurred, or what the nature of the error is.\nPlease be aware that while this information may not be personally identifying by itself, it may be possible to combine it with other data to personally identify individual persons.\nWhen you visit our website or interact with our services, we may automatically collect data about your device, such as:\nData we collect can depend on the individual settings of your device and software. We recommend checking the policies of your device manufacturer or software provider to learn what information they make available to us.\nWe may ask for personal information which may include one or more of the following:\nWe only collect and use your personal information when we have a legitimate reason for doing so. In which instance, we only collect personal information that is reasonably necessary to provide our services to you.\nWe may collect personal information from you when you do any of the following on our website:\nWe may collect, hold, use, and disclose information for the following purposes, and personal information will not be further processed in a manner that is incompatible with these purposes:\nPlease be aware that we may combine information we collect about you with general information or research data we receive from other trusted sources.\nWhen we collect and process personal information, and while we retain this information, we will protect it within commercially acceptable means to prevent loss and theft, as well as unauthorized access, disclosure, copying, use, or modification.\nAlthough we will do our best to protect the personal information you provide to us, we advise that no method of electronic transmission or storage is 100% secure, and no one can guarantee absolute data security. We will comply with laws applicable to us in respect of any data breach.\nWe keep your personal information only for as long as we need to. This time period may depend on what we are using your information for, in accordance with this privacy policy. If your personal information is no longer required, we will delete it or make it anonymous by removing all details that identify you.\nHowever, if necessary, we may retain your personal information for our compliance with a legal, accounting, or reporting obligation or for archiving purposes in the public interest, scientific, or historical research purposes or statistical purposes.\nOur Services and website are not intended for, and should not be used by, children under the age of 18. We do not knowingly collect personal data from children under 18.\nThe personal information we collect is stored and/or processed in United States, or where we or our partners, affiliates, and third-party providers maintain facilities.\nThe countries to which we store, process, or transfer your personal information may not have the same data protection laws as the country in which you initially provided the information. If we transfer your personal information to third parties in other countries: (i) we will perform those transfers in accordance with the requirements of applicable law; and (ii) we will protect the transferred personal information in accordance with this privacy policy.\nYou have certain rights in relation to your personal information. These include: the right to object to the processing of your information for certain purposes, the right to access and rectify your personal information, and the ability to erase, restrict or receive a machine-readable copy of your personal information.\nYou always retain the right to withhold personal information from us, with the understanding that your experience of our website may be affected. We will not discriminate against you for exercising any of your rights over your personal information. If you do provide us with personal information you understand that we will collect, hold, use and disclose it in accordance with this privacy policy. You retain the right to request details of any personal information we hold about you.\nIf we receive personal information about you from a third party, we will protect it as set out in this privacy policy. If you are a third party providing personal information about somebody else, you represent and warrant that you have such person’s consent to provide the personal information to us.\nIf you have previously agreed to us using your personal information for direct marketing purposes, you may change your mind at any time. We will provide you with the ability to unsubscribe from our email-database or opt out of communications. Please be aware we may need to request specific information from you to help us confirm your identity.\nIf you believe that any information we hold about you is inaccurate, out of date, incomplete, irrelevant, or misleading, please contact us using the details provided in this privacy policy. We will take reasonable steps to correct any information found to be inaccurate, incomplete, misleading, or out of date.\nIf you believe that we have breached a relevant data protection law and wish to make a complaint, please contact us using the details below and provide us with full details of the alleged breach. We will promptly investigate your complaint and respond to you, in writing, setting out the outcome of our investigation and the steps we will take to deal with your complaint. You also have the right to contact a regulatory body or data protection authority in relation to your complaint.\nYou have the right to request that we erase your personal data. To exercise this data protection right, email us at the address below.\nAt our discretion, we may change our privacy policy to reflect updates to our business processes, current acceptable practices, or legislative or regulatory changes. If we decide to change this privacy policy, we will post the changes here at the same link by which you are accessing this privacy policy.\nIf the changes are significant, or if required by applicable law, we will contact you (based on your selected preferences for communications from us) and all our registered users with the new details and links to the updated or changed policy.\nIf required by law, we will get your permission or give you the opportunity to opt in to or opt out of, as applicable, any new uses of your personal information.\nWe may change this privacy policy at any time. The new privacy policy will be displayed on our website. The date this privacy policy was last updated appears at the top.\nQuestions and comments regarding this privacy policy should be sent to: \n \nOur registered mailing address is:\nFern\n169 Madison Ave #2324\nNew York New York 10016\nUnited States\n‍\nDelight your developers with SDKs and Docs."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\noverview.mdx\nLatest commit\nHistory\nBreadcrumbs\noverview.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/sound-effects",
    "text": "Overview\nUsage\nPrompting guide\nFAQ\nSound effects\nLearn how to create high-quality sound effects from text with ElevenLabs.\nElevenLabs \n API turns text descriptions into high-quality audio effects with precise control over timing, style and complexity. The model understands both natural language and audio terminology, enabling you to:\nListen to an example:\nSound effects are generated using text descriptions & two optional parameters:\n: Set a specific length for the generated audio (in seconds)\n: Control how strictly the model follows the prompt\nLearn how to integrate sound effects into your application.\nStep-by-step guide for using sound effects in ElevenLabs.\nFor basic sound effects, use clear, concise descriptions:\nFor multi-part sound effects, describe the sequence of events:\nThe API also supports generation of musical components:\nCommon terms that can enhance your prompts:\nThe maximum duration is 30 seconds per generation. For longer sequences, generate multiple\neffects and combine them.\nYes, you can generate musical elements like drum loops, bass lines, and melodic samples.\nHowever, for full music production, consider combining multiple generated elements.\nUse detailed prompts, appropriate duration settings, and high prompt influence for more\npredictable results. For complex sounds, generate components separately and combine them.\nGenerated audio is provided in MP3 format with professional-grade quality (44.1kHz,\n128-192kbps)."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/dubbing",
    "text": "Overview\nUsage\nKey features\nCost\nFAQ\nDubbing\nLearn how to translate audio and video while preserving the emotion, timing & tone of speakers.\nElevenLabs \n API translates audio and video across 32 languages while preserving the emotion, timing, tone and unique characteristics of each speaker. Our model separates each speaker’s dialogue from the soundtrack, allowing you to recreate the original delivery in another language. It can be used to:\nYour browser does not support the video tag.\nWe also offer a \n for video and podcast creators.\nElevenLabs dubbing can be used in two ways:\nThe UI supports files up to \n and \n. The API supports files up to \n and \n.\nLearn how to integrate dubbing into your application.\nEdit transcripts and translate videos step by step in Dubbing Studio.\n\n\nAutomatically detect multiple speakers, even with overlapping speech.\n\n\nGenerate localized tracks in 32 languages.\n\n\nRetain the speaker’s identity and emotional tone.\n\n\nAvoid re-mixing music, effects, or ambient sounds.\n\n\nManually edit translations and transcripts as needed.\n\n\nVideos and audio can be dubbed from various sources, including YouTube, X, TikTok, Vimeo, direct URLs, or file uploads.\n\n\nOur AI video translator lets you manually edit transcripts and translations to ensure your content is properly synced and localized. Adjust the voice settings to tune delivery, and regenerate speech segments until the output sounds just right.\nA Creator plan or higher is required to dub audio files. For videos, a watermark option is\navailable to reduce credit usage.\nTo reduce credit usage, you can:\nRefer to our \n for detailed credit costs.\nDubbing can be performed on all types of short and long form video and audio content. We\nrecommend dubbing content with a maximum of 9 unique speakers at a time to ensure a high-quality\ndub.\nYes. Our models analyze each speaker’s original delivery to recreate the same tone, pace, and\nstyle in your target language.\nWe use advanced source separation to isolate individual voices from ambient sound. Multiple\noverlapping speakers can be split into separate tracks.\nVia the user interface, the maximum file size is 500MB up to 45 minutes. Through the API, you\ncan process files up to 1GB and 2.5 hours.\nYou can choose to dub only certain portions of your video/audio or tweak translations/voices in\nour interactive Dubbing Studio."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/voice-isolator",
    "text": "Overview\nUsage\nSupported file types\nFAQ\nVoice isolator\nLearn how to isolate speech from background noise, music, and ambient sounds from any audio.\nElevenLabs \n API transforms audio recordings with background noise into clean, studio-quality speech. This is particularly useful for audio recorded in noisy environments, or recordings containing unwanted ambient sounds, music, or other background interference.\nListen to a sample:\nThe voice isolator model extracts speech from background noise in both audio and video files.\nLearn how to integrate voice isolator into your application.\nStep-by-step guide for using voice isolator in ElevenLabs."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/voice-changer",
    "text": "Overview\nSupported languages\nBest practices\nAudio quality\nRecording guidelines\nParameters\nFAQ\nVoice changer\nLearn how to transform audio between voices while preserving emotion and delivery.\nElevenLabs \n API lets you transform any source audio (recorded or uploaded) into a different, fully cloned voice without losing the performance nuances of the original. It’s capable of capturing whispers, laughs, cries, accents, and subtle emotional cues to achieve a highly realistic, human feel and can be used to:\nYour browser does not support the video tag.\nExplore our \n to find the perfect voice for your project.\nLearn how to integrate voice changer into your application.\nStep-by-step guide for using voice changer in ElevenLabs.\nOur v2 models support 29 languages:\nThe \n model only supports English.\nYes, but you must split it into smaller chunks (each under 5 minutes). This helps ensure stability\nand consistent output.\nAbsolutely. Provide your custom voice’s \n and specify the correct\n \n\n\n.\nYou’re charged at 1000 characters’ worth of usage per minute of processed audio. There’s no\nadditional fee based on file size.\nPossibly. Use \n or the Voice Isolator tool to minimize\nenvironmental sounds in the final output.\nThough \n is available, our\n \n\n\n model often outperforms it, even for English material.\n“Style” adds interpretative flair; “stability” enforces consistency. For high-energy performances\nin the source audio, turn style down and stability up."
}
,
{
    "url": "https://elevenlabs.io/docs/capabilities/text-to-speech",
    "text": "Overview\nVoice quality\nVoice options\nSupported formats\nSupported languages\nPrompting\nFAQ\nText to Speech\nLearn how to turn text into lifelike spoken audio with ElevenLabs.\nElevenLabs \n API turns text into lifelike audio with nuanced intonation, pacing and emotional awareness. \n adapt to textual cues across 32 languages and multiple voice styles and can be used to:\nListen to a sample:\nExplore our \n to find the perfect voice for your project.\nLearn how to integrate text to speech into your application.\nStep-by-step guide for using text to speech in ElevenLabs.\nFor real-time applications, Flash v2.5 provides ultra-low 75ms latency, while Multilingual v2 delivers the highest quality audio with more nuanced expression.\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nElevenLabs offers thousands of voices across 32 languages through multiple creation methods:\nLearn more about our \n.\nThe default response format is “mp3”, but other formats like “PCM”, & “μ-law” are available.\nHigher quality audio options are only available on paid tiers - see our \n for details.\nOur v2 models support 29 languages:\nFlash v2.5 supports 32 languages - all languages from v2 models plus:\nSimply input text in any of our supported languages and select a matching voice from our \n. For the most natural results, choose a voice with an accent that matches your target language and region.\nThe models interpret emotional context directly from the text input. For example, adding\ndescriptive text like “she said excitedly” or using exclamation marks will influence the speech\nemotion. Voice settings like Stability and Similarity help control the consistency, while the\nunderlying emotion comes from textual cues.\nRead the \n for more details.\nDescriptive text will be spoken out by the model and must be manually trimmed or removed from the\naudio if desired.\nYes, you can create \n of your own voice\nfrom short audio clips. For high-fidelity clones, check out our \n feature.\nYes. You retain ownership of any audio you generate. However, commercial usage rights are only\navailable with paid plans. With a paid subscription, you may use generated audio for commercial\npurposes and monetize the outputs if you own the IP rights to the input content.\nA free regeneration allows you to regenerate the same text to speech content without additional cost, subject to these conditions:\nFree regenerations are useful in case there is a slight distortion in the audio output. According to ElevenLabs’ internal benchmarks, regenerations will solve roughly half of issues with quality, with remaining issues usually due to poor training data.\nUse the low-latency Flash \n (Flash v2 or v2.5) optimized for near real-time\nconversational or interactive scenarios. See our \n for more details.\nThe models are nondeterministic. For consistency, use the optional \n, though subtle\ndifferences may still occur.\nSplit long text into segments and use streaming for real-time playback and efficient processing.\nTo maintain natural prosody flow between chunks, include \n."
}
,
{
    "url": "https://elevenlabs.io/docs/changelog",
    "text": "Changelog\nConversational AI\nVoice Isolator\nWorkspace\nAPI\nConversational AI\nStudio\nEnterprise\nAPI\nSocials\nConversational AI\nReader\nStudio (formerly Projects)\nSDKs\nAPI\nDocs\nConversational AI\nProjects\nAPI\nSDKs\nProduct\nConversational AI\nProduct\nModel\nLaunches\nProduct\nAPI\nProduct\nAPI\nAll \n endpoints have been deprecated in favor of the new \n endpoints. The following endpoints are now deprecated:\nNew endpoints replacing the deprecated \n endpoints\n - New enterprise features for team management\n: Experience Conversational AI in action by talking to Santa this holiday season. For every conversation with santa we donate 2 dollars to \n (up to $11,000).\n: Get $50+ in credits from leading AI developer tools, including ElevenLabs.\n: Launched in the ElevenReader app. \n: Now generally available to all customers. \n: The website TTS redesign is now rolled out to all customers.\n: Now available in Projects. \n:\n:"
}
,
{
    "url": "https://github.com/fern-api/fern",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nLicense\nfern-api/fern\nFolders and files\nLatest commit\nHistory\nRepository files navigation\n🌿 SDKs\n🌿 API Documentation\n🌿 Generators\nSDK Generators\nServer-side Generators\nModel Generators\nSpec Generators\n🌿 CLI Commands\nAdvanced\nAPI First\nFern Definition\nInspiration\nCommunity\nContributing\nAbout\nTopics\nResources\nLicense\nStars\nWatchers\nForks\n\n  \n\n  \nLanguages\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \n\n        Input OpenAPI. Output SDKs and Docs.\n      \n\n\n\n\nFern is a toolkit that allows you to input your API Definition and output SDKs and API documentation. Fern is compatible with the OpenAPI specification (formerly Swagger).\nThe Fern toolkit is available via a command line interface (CLI) and requires Node 18+. To install it, run:\nInitialize Fern with your OpenAPI spec:\nYour directory should look like the following:\nFinally, to invoke the generator, run:\n🎉 Once the command completes, you'll see your SDK in \n.\nFern can also build and host a documentation website with an auto-generated API reference. Write additional pages in markdown and have them versioned with git. Search, SEO, dark mode, and popular components are provided out-of-the-box. Plus, you can customize the colors, font, logo, and domain name.\nCheck out docs built with Fern:\nGet started \n.\nGenerators are process that take your API Definition as input and output artifacts (SDKs,\nPostman Collections, Server boilerplate, etc.). To add a generator run \nFern's server-side generators output boilerplate application code (models and networking logic). This is intended for spec-first or API-first developers, who write their API definition (as an OpenAPI spec or Fern definition) and want to generate backend code.\nFern's model generators will output schemas or types defined in your OpenAPI spec or Fern Definition.\nFern's spec generators can output an OpenAPI spec or a Postman collection.\n: The OpenAPI spec generator is primarily intended for Fern Definition users. This prevents lock-in so that one can always export to OpenAPI.\nHere's a quick look at the most popular CLI commands. View the documentation for \n.\n: adds a new starter API to your repository.\n: validate your API definition and Fern configuration.\n: run the generators specified in \n in the cloud.\n: run the generators specified in \n in docker locally.\n: include a new generator in your \n. For example, \n.\nFern supports developers and teams that want to be API-first or Spec-first.\nDefine your API, and use Fern to generate models, networking code and boilerplate application code. The generated code adds\ntype safety to your API implementation - if your backend doesn't implement the API correctly, it won't compile.\nFrameworks currently supported:\nWhile we are big fans of OpenAPI, we know it isn't the \n format to read and write. If you're looking for an alternative,\ngive the Fern Definition a try.\nInstall the Fern CLI and initialize a Fern Project.\nThis will create the following folder structure in your project:\nHere's what the \n starter file looks like:\nCheckout open source projects that are using Fern Definitions:\nFern is inspired by internal tooling built to enhance the developer experience. We stand on the shoulders of giants. While teams were responsible for building the following tools, we want to give a shout out to Mark Elliot (creator of Conjure at Palantir), Michael Dowling (creator of Smithy at AWS), and Ian McCrystal (creator of Stripe Docs).\n We are here to answer questions and help you get the most out of Fern.\nWe welcome community contributions. For guidelines, refer to our \n.\n\n        Input OpenAPI. Output SDKs and Docs."
}
,
{
    "url": "https://docs.github.com/en/site-policy/privacy-policies/github-general-privacy-statement",
    "text": "GitHub General Privacy Statement\nIn this article\nHelp and support\nHelp us make these docs great!\nStill need help?\nLegal\nEffective date: February 1, 2024\nWelcome to the GitHub Privacy Statement. This is where we describe how we handle your “Personal Data”, which is information that is directly linked or can be linked to you. It applies to the Personal Data that GitHub, Inc. or GitHub B.V., processes as the “Data Controller” when you interact with websites, applications, and services that display this Statement (collectively, “Services”). This Statement does not apply to services or products that do not display this Statement, such as Previews, where relevant.\nWhen a school or employer supplies your GitHub account, they assume the role of Data Controller for most Personal Data used in our Services. This enables them to:\nShould you access a GitHub Service through an account provided by an organization, such as your employer or school, the organization becomes the Data Controller, and this Privacy Statement's direct applicability to you changes. Even so, GitHub remains dedicated to preserving your privacy rights. In such circumstances, GitHub functions as a Data Processor, adhering to the Data Controller's instructions regarding your Personal Data's processing. A Data Protection Agreement governs the relationship between GitHub and the Data Controller. For further details regarding their privacy practices, please refer to the privacy statement of the organization providing your account.\nIn cases where your organization grants access to GitHub products, GitHub acts as the Data Controller solely for specific processing activities. These activities are clearly defined in a contractual agreement with your organization, known as a Data Protection Agreement. You can review our standard Data Protection Agreement at \n. For those limited purposes, this Statement governs the handling of your Personal Data. For all other aspects of GitHub product usage, your organization's policies apply.\nWhen you use third-party extensions, integrations, or follow references and links within our Services, the privacy policies of these third parties apply to any Personal Data you provide or consent to share with them. Their privacy statements will govern how this data is processed.\nPersonal Data is collected from you directly, automatically from your device, and also from third parties. The Personal Data GitHub processes when you use the Services depends on variables like how you interact with our Services (such as through web interfaces, desktop or mobile applications), the features you use (such as pull requests, Codespaces, or GitHub Copilot) and your method of accessing the Services (your preferred IDE). Below, we detail the information we collect through each of these channels:\nThe Personal Data we process depends on your interaction and access methods with our Services, including the interfaces (web, desktop, mobile apps), features used (pull requests, Codespaces, GitHub Copilot), and your preferred access tools (like your IDE). This section details all the potential ways GitHub may process your Personal Data:\nWhen carrying out these activities, GitHub practices data minimization and uses the minimum amount of Personal Information required.\nWe may share Personal Data with the following recipients:\nIf your GitHub account has private repositories, you control the access to that information. GitHub personnel does not access private repository information without your consent except as provided in this Privacy Statement and for:\nGitHub will provide you with notice regarding private repository access unless doing so is prohibited by law or if GitHub acted in response to a security threat or other risk to security.\nGitHub processes Personal Data in compliance with the GDPR, ensuring a lawful basis for each processing activity. The basis varies depending on the data type and the context, including how you access the services. Our processing activities typically fall under these lawful bases:\nDepending on your residence location, you may have specific legal rights regarding your Personal Data:\nTo exercise these rights, please send an email to privacy[at]github[dot]com and follow the instructions provided. To verify your identity for security, we may request extra information before addressing your data-related request. Please contact our Data Protection Officer at dpo[at]github[dot]com for any feedback or concerns. Depending on your region, you have the right to complain to your local Data Protection Authority. European users can find authority contacts on the European Data Protection Board website, and UK users on the Information Commissioner’s Office website.\nWe aim to promptly respond to requests in compliance with legal requirements. Please note that we may retain certain data as necessary for legal obligations or for establishing, exercising, or defending legal claims.\nGitHub stores and processes Personal Data in a variety of locations, including your local region, the United States, and other countries where GitHub, its affiliates, subsidiaries, or subprocessors have operations. We transfer Personal Data from the European Union, the United Kingdom, and Switzerland to countries that the European Commission has not recognized as having an adequate level of data protection. When we engage in such transfers, we generally rely on the standard contractual clauses published by the European Commission under \n, to help protect your rights and enable these protections to travel with your data. To learn more about the European Commission’s decisions on the adequacy of the protection of personal data in the countries where GitHub processes personal data, see this article on the \n.\nGitHub also complies with the EU-U.S. Data Privacy Framework (EU-U.S. DPF), the UK Extension to the EU-U.S. DPF, and the Swiss-U.S. Data Privacy Framework (Swiss-U.S. DPF) as set forth by the U.S. Department of Commerce. GitHub has certified to the U.S. Department of Commerce that it adheres to the EU-U.S. Data Privacy Framework Principles (EU-U.S. DPF Principles) with regard to the processing of personal data received from the European Union in reliance on the EU-U.S. DPF and from the United Kingdom (and Gibraltar) in reliance on the UK Extension to the EU-U.S. DPF. GitHub has certified to the U.S. Department of Commerce that it adheres to the Swiss-U.S. Data Privacy Framework Principles (Swiss-U.S. DPF Principles) with regard to the processing of personal data received from Switzerland in reliance on the Swiss-U.S. DPF. If there is any conflict between the terms in this privacy statement and the EU-U.S. DPF Principles and/or the Swiss-U.S. DPF Principles, the Principles shall govern. To learn more about the Data Privacy Framework (DPF) program, and to view our certification, please visit \n.\nGitHub has the responsibility for the processing of Personal Data it receives under the Data Privacy Framework (DPF) Principles and subsequently transfers to a third party acting as an agent on GitHub’s behalf. GitHub shall remain liable under the DPF Principles if its agent processes such Personal Data in a manner inconsistent with the DPF Principles, unless the organization proves that it is not responsible for the event giving rise to the damage.\nIn compliance with the EU-U.S. DPF, the UK Extension to the EU-U.S. DPF, and the Swiss-U.S. DPF, GitHub commits to resolve DPF Principles-related complaints about our collection and use of your personal information. EU, UK, and Swiss individuals with inquiries or complaints regarding our handling of personal data received in reliance on the EU-U.S. DPF, the UK Extension, and the Swiss-U.S. DPF should first contact GitHub at: dpo[at]github[dot]com.\nIf you do not receive timely acknowledgment of your DPF Principles-related complaint from us, or if we have not addressed your DPF Principles-related complaint to your satisfaction, please visit \n for more information or to file a complaint. The services of the International Centre for Dispute Resolution are provided at no cost to you.\nAn individual has the possibility, under certain conditions, to invoke binding arbitration for complaints regarding DPF compliance not resolved by any of the other DPF mechanisms. For additional information visit \n.\nGitHub is subject to the investigatory and enforcement powers of the Federal Trade Commission (FTC). Under Section 5 of the Federal Trade Commission Act (15 U.S.C. § 45), an organization's failure to abide by commitments to implement the DPF Principles may be challenged as deceptive by the FTC. The FTC has the power to prohibit such misrepresentations through administrative orders or by seeking court orders.\nGitHub uses appropriate administrative, technical, and physical security controls to protect your Personal Data. We’ll retain your Personal Data as long as your account is active and as needed to fulfill contractual obligations, comply with legal requirements, resolve disputes, and enforce agreements. The retention duration depends on the purpose of data collection and any legal obligations.\nGitHub uses administrative, technical, and physical security controls where appropriate to protect your Personal Data.\nContact us via our contact form or by emailing our Data Protection Officer at dpo[at]github[dot]com.\nOur addresses are:\nGitHub B.V.\nPrins Bernhardplein 200, Amsterdam\n1097JB\nThe Netherlands\nGitHub, Inc.\n88 Colin P. Kelly Jr. St.\nSan Francisco, CA 94107\nUnited States\nOur Services are not intended for individuals under the age of 13. We do not intentionally gather Personal Data from such individuals. If you become aware that a minor has provided us with Personal Data, please \n.\nGitHub may periodically revise this Privacy Statement. If there are material changes to the statement, we will provide at least 30 days prior notice by updating our website or sending an email to your primary email address associated with your GitHub account.\nBelow are translations of this document into other languages. In the event of any conflict, uncertainty, or apparent inconsistency between any of those versions and the English version, this English version is the controlling version.\nCliquez ici pour obtenir la version française: \n.\nFor translations of this statement into other languages, please visit \n and select a language from the drop-down menu under “English.”\nGitHub uses cookies to provide, secure and improve our Service or to develop new features and functionality of our Service. For example, we use them to (i) keep you logged in, (ii) remember your preferences, (iii) identify your device for security and fraud purposes, including as needed to maintain the integrity of our Service, (iv) compile statistical reports, and (v) provide information and insight for future development of GitHub. We provide more information about \n that describes the cookies we set, the needs we have for those cookies, and the expiration of such cookies.\nFor Enterprise Marketing Pages, we may also use non-essential cookies to (i) gather information about enterprise users’ interests and online activities to personalize their experiences, including by making the ads, content, recommendations, and marketing seen or received more relevant and (ii) serve and measure the effectiveness of targeted advertising and other marketing efforts. If you disable the non-essential cookies on the Enterprise Marketing Pages, the ads, content, and marketing you see may be less relevant.\nOur emails to users may contain a pixel tag, which is a small, clear image that can tell us whether or not you have opened an email and what your IP address is. We use this pixel tag to make our email communications more effective and to make sure we are not sending you unwanted email.\nThe length of time a cookie will stay on your browser or device depends on whether it is a “persistent” or “session” cookie. Session cookies will only stay on your device until you stop browsing. Persistent cookies stay until they expire or are deleted. The expiration time or retention period applicable to persistent cookies depends on the purpose of the cookie collection and tool used. You may be able to delete cookie data. For more information, see \n.\nWe use cookies and similar technologies, such as web beacons, local storage, and mobile analytics, to operate and provide our Services. When visiting Enterprise Marketing Pages, like resources.github.com, these and additional cookies, like advertising IDs, may be used for sales and marketing purposes.\nCookies are small text files stored by your browser on your device. A cookie can later be read when your browser connects to a web server in the same domain that placed the cookie. The text in a cookie contains a string of numbers and letters that may uniquely identify your device and can contain other information as well. This allows the web server to recognize your browser over time, each time it connects to that web server.\nWeb beacons are electronic images (also called “single-pixel” or “clear GIFs”) that are contained within a website or email. When your browser opens a webpage or email that contains a web beacon, it automatically connects to the web server that hosts the image (typically operated by a third party). This allows that web server to log information about your device and to set and read its own cookies. In the same way, third-party content on our websites (such as embedded videos, plug-ins, or ads) results in your browser connecting to the third-party web server that hosts that content.\nMobile identifiers for analytics can be accessed and used by apps on mobile devices in much the same way that websites access and use cookies. When visiting Enterprise Marketing pages, like resources.github.com, on a mobile device these may allow us and our third-party analytics and advertising partners to collect data for sales and marketing purposes.\nWe may also use so-called “flash cookies” (also known as “Local Shared Objects” or “LSOs”) to collect and store information about your use of our Services. Flash cookies are commonly used for advertisements and videos.\nThe GitHub Services use cookies and similar technologies for a variety of purposes, including to store your preferences and settings, enable you to sign-in, analyze how our Services perform, track your interaction with the Services, develop inferences, combat fraud, and fulfill other legitimate purposes. Some of these cookies and technologies may be provided by third parties, including service providers and advertising partners. For example, our analytics and advertising partners may use these technologies in our Services to collect personal information (such as the pages you visit, the links you click on, and similar usage information, identifiers, and device information) related to your online activities over time and across Services for various purposes, including targeted advertising. GitHub will place non-essential cookies on pages where we market products and services to enterprise customers, for example, on resources.github.com.\nWe and/or our partners also share the information we collect or infer with third parties for these purposes.\nThe table below provides additional information about how we use different types of cookies:\nYou have several options to disable non-essential cookies:\nAny GitHub page that serves non-essential cookies will have a link in the page’s footer to cookie settings. You can express your preferences at any time by clicking on that linking and updating your settings.\nSome users will also be able to manage non-essential cookies via a cookie consent banner, including the options to accept, manage, and reject all non-essential cookies.\n\nYou can control the cookies you encounter on the web using a variety of widely-available tools. For example:\nThese choices are specific to the browser you are using. If you access our Services from other devices or browsers, take these actions from those systems to ensure your choices apply to the data collected when you use those systems.\nThis section provides extra information specifically for residents of certain US states that have distinct data privacy laws and regulations. These laws may grant specific rights to residents of these states when the laws come into effect. This section uses the term “personal information” as an equivalent to the term “Personal Data.”\nThese rights are common to the US State privacy laws:\nWe may collect various categories of personal information about our website visitors and users of \"Services\" which includes GitHub applications, software, products, or services. That information includes identifiers/contact information, demographic information, payment information, commercial information, internet or electronic network activity information, geolocation data, audio, electronic, visual, or similar information, and inferences drawn from such information.\nWe collect this information for various purposes. This includes identifying accessibility gaps and offering targeted support, fostering diversity and representation, providing services, troubleshooting, conducting business operations such as billing and security, improving products and supporting research, communicating important information, ensuring personalized experiences, and promoting safety and security.\nTo make an access, deletion, correction, or opt-out request, please send an email to privacy[at]github[dot]com and follow the instructions provided. We may need to verify your identity before processing your request. If you choose to use an authorized agent to submit a request on your behalf, please ensure they have your signed permission or power of attorney as required.\nTo opt out of the sharing of your personal information, you can click on the \"Do Not Share My Personal Information\" link on the footer of our Websites or use the Global Privacy Control (\"GPC\") if available. Authorized agents can also submit opt-out requests on your behalf.\nWe also make the following disclosures for purposes of compliance with California privacy law:\nUnder California Civil Code section 1798.83, also known as the “Shine the Light” law, California residents who have provided personal information to a business with which the individual has established a business relationship for personal, family, or household purposes (“California Customers”) may request information about whether the business has disclosed personal information to any third parties for the third parties’ direct marketing purposes. Please be aware that we do not disclose personal information to any third parties for their direct marketing purposes as defined by this law. California Customers may request further information about our compliance with this law by emailing (privacy[at]github[dot]com). Please note that businesses are required to respond to one request per California Customer each year and may not be required to respond to requests made by means other than through the designated email address.\nCalifornia residents under the age of 18 who are registered users of online sites, services, or applications have a right under California Business and Professions Code Section 22581 to remove, or request and obtain removal of, content or information they have publicly posted. To remove content or information you have publicly posted, please submit a \n. Alternatively, to request that we remove such content or information, please send a detailed description of the specific content or information you wish to have removed to \n. Please be aware that your request does not guarantee complete or comprehensive removal of content or information posted online and that the law may not permit or require removal in certain circumstances. If you have any questions about our privacy practices with respect to California residents, please send an email to privacy[at]github[dot]com.\nWe value the trust you place in us and are committed to handling your personal information with care and respect. If you have any questions or concerns about our privacy practices, please email our Data Protection Officer at dpo[at]github[dot]com.\nIf you live in Colorado, Connecticut, or Virginia you have some additional rights:\nWe do not sell your covered information, as defined under Chapter 603A of the Nevada Revised Statutes. If you still have questions about your covered information or anything else in our Privacy Statement, please send an email to privacy[at]github[dot]com.\nAll GitHub docs are open source. See something that's wrong or unclear? Submit a pull request."
}
,
{
    "url": "https://docs.github.com/en/site-policy/github-terms/github-terms-of-service",
    "text": "GitHub Terms of Service\nIn this article\nHelp and support\nHelp us make these docs great!\nStill need help?\nLegal\nThank you for using GitHub! We're happy you're here. Please read this Terms of Service agreement carefully before accessing or using GitHub. Because it is such an important contract between us and our users, we have tried to make it as clear as possible. For your convenience, we have presented these terms in a short non-binding summary followed by the full legal terms.\nEffective date: November 16, 2020\n \n \nUsers. Subject to these Terms, you retain ultimate administrative control over your Personal Account and the Content within it.\nOrganizations. The \"owner\" of an Organization that was created under these Terms has ultimate administrative control over that Organization and the Content within it. Within the Service, an owner can manage User access to the Organization’s data and projects. An Organization may have multiple owners, but there must be at least one Personal Account designated as an owner of an Organization. If you are the owner of an Organization under these Terms, we consider you responsible for the actions that are performed on or through that Organization.\nYou must provide a valid email address in order to complete the signup process. Any other information requested, such as your real name, is optional, unless you are accepting these terms on behalf of a legal entity (in which case we need more information about the legal entity) or if you opt for a \n, in which case additional information will be necessary for billing purposes.\nWe have a few simple rules for Personal Accounts on GitHub's Service.\nYou are responsible for keeping your Account secure while you use our Service. We offer tools such as two-factor authentication to help you maintain your Account's security, but the content of your Account and its security are up to you.\nIn some situations, third parties' terms may apply to your use of GitHub. For example, you may be a member of an organization on GitHub with its own terms or license agreements; you may download an application that integrates with GitHub; or you may use GitHub to authenticate to another service. Please be aware that while these Terms are our full agreement with you, other parties' terms govern their relationships with you.\nIf you are a government User or otherwise accessing or using any GitHub Service in a government capacity, this \n applies to you, and you agree to its provisions.\nIf you have signed up for GitHub Enterprise Cloud, the \n applies to you, and you agree to its provisions.\n \nYour use of the Website and Service must not violate any applicable laws, including copyright or trademark laws, export control or sanctions laws, or other laws in your jurisdiction. You are responsible for making sure that your use of the Service is in compliance with laws and any applicable regulations.\nYou agree that you will not under any circumstances violate our \n or \n.\n \nYou may create or upload User-Generated Content while using the Service. You are solely responsible for the content of, and for any harm resulting from, any User-Generated Content that you post, upload, link to or otherwise make available via the Service, regardless of the form of that Content. We are not responsible for any public display or misuse of your User-Generated Content.\nWe have the right to refuse or remove any User-Generated Content that, in our sole discretion, violates any laws or \n. User-Generated Content displayed on GitHub Mobile may be subject to mobile app stores' additional terms.\nYou retain ownership of and responsibility for Your Content. If you're posting anything you did not create yourself or do not own the rights to, you agree that you are responsible for any Content you post; that you will only submit Content that you have the right to post; and that you will fully comply with any third party licenses relating to Content you post.\nBecause you retain ownership of and responsibility for Your Content, we need you to grant us — and other GitHub Users — certain legal permissions, listed in Sections D.4 — D.7. These license grants apply to Your Content. If you upload Content that already comes with a license granting GitHub the permissions we need to run our Service, no additional license is required. You understand that you will not receive any payment for any of the rights granted in Sections D.4 — D.7. The licenses you grant to us will end when you remove Your Content from our servers, unless other Users have forked it.\nWe need the legal right to do things like host Your Content, publish it, and share it. You grant us and our legal successors the right to store, archive, parse, and display Your Content, and make incidental copies, as necessary to provide the Service, including improving the Service over time. This license includes the right to do things like copy it to our database and make backups; show it to you and other users; parse it into a search index or otherwise analyze it on our servers; share it with other users; and perform it, in case Your Content is something like music or video.\nThis license does not grant GitHub the right to sell Your Content. It also does not grant GitHub the right to otherwise distribute or use Your Content outside of our provision of the Service, except that as part of the right to archive Your Content, GitHub may permit our partners to store and archive Your Content in public repositories in connection with the \n.\nAny User-Generated Content you post publicly, including issues, comments, and contributions to other Users' repositories, may be viewed by others. By setting your repositories to be viewed publicly, you agree to allow others to view and \"fork\" your repositories (this means that others may make their own copies of Content from your repositories in repositories they control).\nIf you set your pages and repositories to be viewed publicly, you grant each User of GitHub a nonexclusive, worldwide license to use, display, and perform Your Content through the GitHub Service and to reproduce Your Content solely on GitHub as permitted through GitHub's functionality (for example, through forking). You may grant further rights if you \n. If you are uploading Content you did not create or own, you are responsible for ensuring that the Content you upload is licensed under terms that grant these permissions to other GitHub Users.\nWhenever you add Content to a repository containing notice of a license, you license that Content under the same terms, and you agree that you have the right to license that Content under those terms. If you have a separate agreement to license that Content under different terms, such as a contributor license agreement, that agreement will supersede.\nIsn't this just how it works already? Yep. This is widely accepted as the norm in the open-source community; it's commonly referred to by the shorthand \"inbound=outbound\". We're just making it explicit.\nYou retain all moral rights to Your Content that you upload, publish, or submit to any part of the Service, including the rights of integrity and attribution. However, you waive these rights and agree not to assert them against us, to enable us to reasonably exercise the rights granted in Section D.4, but not otherwise.\nTo the extent this agreement is not enforceable by applicable law, you grant GitHub the rights we need to use Your Content without attribution and to make reasonable adaptations of Your Content as necessary to render the Website and provide the Service.\n \nSome Accounts may have private repositories, which allow the User to control access to Content.\nGitHub considers the contents of private repositories to be confidential to you. GitHub will protect the contents of private repositories from unauthorized use, access, or disclosure in the same manner that we would use to protect our own confidential information of a similar nature and in no event with less than a reasonable degree of care.\nGitHub personnel may only access the content of your private repositories in the situations described in our \n.\nYou may choose to enable additional access to your private repositories. For example:\nAdditionally, we may be \n to disclose the contents of your private repositories.\nGitHub will provide notice regarding our access to private repository content, unless \n, to comply with our legal obligations, or where otherwise bound by requirements under law, for automated scanning, or if in response to a security threat or other risk to security.\nIf you believe that content on our website violates your copyright, please contact us in accordance with our \n. If you are a copyright owner and you believe that content on GitHub violates your rights, please contact us via \n or by emailing \n. There may be legal consequences for sending a false or frivolous takedown notice. Before sending a takedown request, you must consider legal uses such as fair use and licensed uses.\nWe will terminate the Accounts of \n of this policy.\n \nGitHub and our licensors, vendors, agents, and/or our content providers retain ownership of all intellectual property rights of any kind related to the Website and Service. We reserve all rights that are not expressly granted to you under this Agreement or by law. The look and feel of the Website and Service is copyright © GitHub, Inc. All rights reserved. You may not duplicate, copy, or reuse any portion of the HTML/CSS, JavaScript, or visual design elements or concepts without express written permission from GitHub.\nIf you’d like to use GitHub’s trademarks, you must follow all of our trademark guidelines, including those on our logos page: \n.\nThis Agreement is licensed under this \n. For details, see our \n.\n \nAbuse or excessively frequent requests to GitHub via the API may result in the temporary or permanent suspension of your Account's access to the API. GitHub, in our sole discretion, will determine abuse or excessive usage of the API. We will make a reasonable attempt to warn you via email prior to suspension.\nYou may not share API tokens to exceed GitHub's rate limitations.\nYou may not use the API to download data or Content from GitHub for spamming purposes, including for the purposes of selling GitHub users' personal information, such as to recruiters, headhunters, and job boards.\nAll use of the GitHub API is subject to these Terms of Service and the \n.\nGitHub may offer subscription-based access to our API for those Users who require high-throughput access or access that would result in resale of GitHub's Service.\n \nSome Service features may be subject to additional terms specific to that feature or product as set forth in the GitHub Additional Product Terms. By accessing or using the Services, you also agree to the \n.\n \nBeta Previews may not be supported and may be changed at any time without notice. In addition, Beta Previews are not subject to the same security measures and auditing to which the Service has been and is subject. \nAs a user of Beta Previews, you may get access to special information that isn’t available to the rest of the world. Due to the sensitive nature of this information, it’s important for us to make sure that you keep that information secret.\n You agree that any non-public Beta Preview information we give you, such as information about a private Beta Preview, will be considered GitHub’s confidential information (collectively, “Confidential Information”), regardless of whether it is marked or identified as such. You agree to only use such Confidential Information for the express purpose of testing and evaluating the Beta Preview (the “Purpose”), and not for any other purpose. You should use the same degree of care as you would with your own confidential information, but no less than reasonable precautions to prevent any unauthorized use, disclosure, publication, or dissemination of our Confidential Information. You promise not to disclose, publish, or disseminate any Confidential Information to any third party, unless we don’t otherwise prohibit or restrict such disclosure (for example, you might be part of a GitHub-organized group discussion about a private Beta Preview feature).\n Confidential Information will not include information that is: (a) or becomes publicly available without breach of this Agreement through no act or inaction on your part (such as when a private Beta Preview becomes a public Beta Preview); (b) known to you before we disclose it to you; (c) independently developed by you without breach of any confidentiality obligation to us or any third party; or (d) disclosed with permission from GitHub. You will not violate the terms of this Agreement if you are required to disclose Confidential Information pursuant to operation of law, provided GitHub has been given reasonable advance written notice to object, unless prohibited by law.\nWe’re always trying to improve of products and services, and your feedback as a Beta Preview user will help us do that. If you choose to give us any ideas, know-how, algorithms, code contributions, suggestions, enhancement requests, recommendations or any other feedback for our products or services (collectively, “Feedback”), you acknowledge and agree that GitHub will have a royalty-free, fully paid-up, worldwide, transferable, sub-licensable, irrevocable and perpetual license to implement, use, modify, commercially exploit and/or incorporate the Feedback into our products, services, and documentation.\n \nOur pricing and payment terms are available at \n. If you agree to a subscription price, that will remain your price for the duration of the payment term; however, prices are subject to change at the end of a payment term.\n For monthly or yearly payment plans, the Service is billed in advance on a monthly or yearly basis respectively and is non-refundable. There will be no refunds or credits for partial months of service, downgrade refunds, or refunds for months unused with an open Account; however, the service will remain active for the length of the paid billing period. In order to treat everyone equally, no exceptions will be made.\n Some Service features are billed based on your usage. A limited quantity of these Service features may be included in your plan for a limited term without additional charge. If you choose to use paid Service features beyond the quantity included in your plan, you pay for those Service features based on your actual usage in the preceding month. Monthly payment for these purchases will be charged on a periodic basis in arrears. See \n.\n For invoiced Users, User agrees to pay the fees in full, up front without deduction or setoff of any kind, in U.S. Dollars. User must pay the fees within thirty (30) days of the GitHub invoice date. Amounts payable under this Agreement are non-refundable, except as otherwise provided in this Agreement. If User fails to pay any fees on time, GitHub reserves the right, in addition to taking any other action at law or equity, to (i) charge interest on past due amounts at 1.0% per month or the highest interest rate allowed by law, whichever is less, and to charge all expenses of recovery, and (ii) terminate the applicable order form. User is solely responsible for all taxes, fees, duties and governmental assessments (except for taxes based on GitHub's net income) that are imposed or become due in connection with this Agreement.\nBy agreeing to these Terms, you are giving us permission to charge your on-file credit card, PayPal account, or other approved methods of payment for fees that you authorize for GitHub.\nYou are responsible for all fees, including taxes, associated with your use of the Service. By using the Service, you agree to pay GitHub any charge incurred in connection with your use of the Service. If you dispute the matter, contact us through the \n. You are responsible for providing us with a valid means of payment for paid Accounts. Free Accounts are not required to provide payment information.\n \nIt is your responsibility to properly cancel your Account with GitHub. You can \n by going into your Settings in the global navigation bar at the top of the screen. The Account screen provides a simple, no questions asked cancellation link. We are not able to cancel Accounts in response to an email or phone request.\nWe will retain and use your information as necessary to comply with our legal obligations, resolve disputes, and enforce our agreements, but barring legal requirements, we will delete your full profile and the Content of your repositories within 90 days of cancellation or termination (though some information may remain in encrypted backups). This information cannot be recovered once your Account is canceled.\nWe will not delete Content that you have contributed to other Users' repositories or that other Users have forked.\nUpon request, we will make a reasonable effort to provide an Account owner with a copy of your lawful, non-infringing Account contents after Account cancellation, termination, or downgrade. You must make this request within 90 days of cancellation, termination, or downgrade.\nGitHub has the right to suspend or terminate your access to all or any part of the Website at any time, with or without cause, with or without notice, effective immediately. GitHub reserves the right to refuse service to anyone for any reason at any time.\nAll provisions of this Agreement which, by their nature, should survive termination \n survive termination — including, without limitation: ownership provisions, warranty disclaimers, indemnity, and limitations of liability.\n \nFor contractual purposes, you (1) consent to receive communications from us in an electronic form via the email address you have submitted or via the Service; and (2) agree that all Terms of Service, agreements, notices, disclosures, and other communications that we provide to you electronically satisfy any legal requirement that those communications would satisfy if they were on paper. This section does not affect your non-waivable rights.\nCommunications made through email or GitHub Support's messaging system will not constitute legal notice to GitHub or any of its officers, employees, agents or representatives in any situation where notice to GitHub is required by contract or any law or regulation. Legal notice to GitHub must be in writing and \n.\nGitHub only offers support via email, in-Service communications, and electronic messages. We do not offer telephone support.\n \nGitHub provides the Website and the Service “as is” and “as available,” without warranty of any kind. Without limiting this, we expressly disclaim all warranties, whether express, implied or statutory, regarding the Website and the Service including without limitation any warranty of merchantability, fitness for a particular purpose, title, security, accuracy and non-infringement.\nGitHub does not warrant that the Service will meet your requirements; that the Service will be uninterrupted, timely, secure, or error-free; that the information provided through the Service is accurate, reliable or correct; that any defects or errors will be corrected; that the Service will be available at any particular time or location; or that the Service is free of viruses or other harmful components. You assume full responsibility and risk of loss resulting from your downloading and/or use of files, information, content or other material obtained from the Service.\n \nYou understand and agree that we will not be liable to you or any third party for any loss of profits, use, goodwill, or data, or for any incidental, indirect, special, consequential or exemplary damages, however arising, that result from\nOur liability is limited whether or not we have been informed of the possibility of such damages, and even if a remedy set forth in this Agreement is found to have failed of its essential purpose. We will have no liability for any failure or delay due to matters beyond our reasonable control.\n \nIf you have a dispute with one or more Users, you agree to release GitHub from any and all claims, demands and damages (actual and consequential) of every kind and nature, known and unknown, arising out of or in any way connected with such disputes.\nYou agree to indemnify us, defend us, and hold us harmless from and against any and all claims, liabilities, and expenses, including attorneys’ fees, arising out of your use of the Website and the Service, including but not limited to your violation of this Agreement, provided that GitHub (1) promptly gives you written notice of the claim, demand, suit or proceeding; (2) gives you sole control of the defense and settlement of the claim, demand, suit or proceeding (provided that you may not settle any claim, demand, suit or proceeding unless the settlement unconditionally releases GitHub of all liability); and (3) provides to you all reasonable assistance, at your expense.\n \nWe reserve the right, at our sole discretion, to amend these Terms of Service at any time and will update these Terms of Service in the event of any such amendments. We will notify our Users of material changes to this Agreement, such as price increases, at least 30 days prior to the change taking effect by posting a notice on our Website or sending email to the primary email address specified in your GitHub account. Customer's continued use of the Service after those 30 days constitutes agreement to those revisions of this Agreement. For any other modifications, your continued use of the Website constitutes agreement to our revisions of these Terms of Service. You can view all changes to these Terms in our \n repository.\nWe reserve the right at any time and from time to time to modify or discontinue, temporarily or permanently, the Website (or any part of it) with or without notice.\nExcept to the extent applicable law provides otherwise, this Agreement between you and GitHub and any access to or use of the Website or the Service are governed by the federal laws of the United States of America and the laws of the State of California, without regard to conflict of law provisions. You and GitHub agree to submit to the exclusive jurisdiction and venue of the courts located in the City and County of San Francisco, California.\nGitHub may assign or delegate these Terms of Service and/or the \n, in whole or in part, to any person or entity at any time with or without your consent, including the license grant in Section D.4. You may not assign or delegate any rights or obligations under the Terms of Service or Privacy Statement without our prior written consent, and any unauthorized assignment and delegation by you is void.\nThroughout this Agreement, each section includes titles and brief summaries of the following terms and conditions. These section titles and brief summaries are not legally binding.\nIf any part of this Agreement is held invalid or unenforceable, that portion of the Agreement will be construed to reflect the parties’ original intent. The remaining portions will remain in full force and effect. Any failure on the part of GitHub to enforce any provision of this Agreement will not be considered a waiver of our right to enforce such provision. Our rights under this Agreement will survive any termination of this Agreement.\nThis Agreement may only be modified by a written amendment signed by an authorized representative of GitHub, or by the posting by GitHub of a revised version in accordance with \n. These Terms of Service, together with the GitHub Privacy Statement, represent the complete and exclusive statement of the agreement between you and us. This Agreement supersedes any proposal or prior agreement oral or written, and any other communications between you and GitHub relating to the subject matter of these terms including any confidentiality or nondisclosure agreements.\nQuestions about the Terms of Service? Contact us through the \n.\nAll GitHub docs are open source. See something that's wrong or unclear? Submit a pull request."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nprompting.mdx\nLatest commit\nHistory\nBreadcrumbs\nprompting.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://buildwithfern.com/learn/docs/getting-started/overview",
    "text": "Key Features\nFern Docs Overview\nA static site generator for developer-focused companies to build and host a beautiful, professional docs site.\nFern Docs provides versatile documentation to support all your content needs, including popular formats like:\nMatch your docs to your brand with customizable colors, fonts, and styles.\nAuto-generate API docs from OpenAPI and AsyncAPI specs.\nVersion and manage docs with your Git provider using Markdown or MDX.\nTest APIs directly in the docs with auto-populated credentials.\nExample code auto-updates as your SDKs evolve.\nUse pre-built or custom React components for a polished look."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nzero-retention.mdx\nLatest commit\nHistory\nBreadcrumbs\nzero-retention.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/error-messages.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nerror-messages.mdx\nLatest commit\nHistory\nBreadcrumbs\nerror-messages.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://buildwithfern.com/learn/api-definition/openapi/overview",
    "text": "Setup your Fern folder\nWhat is an OpenAPI Specification?\nOpenAPI is a standard for documenting REST APIs\nThe OpenAPI Specification (OAS) is a framework used by developers to document REST APIs. The specification\r\nwritten in JSON or YAML and contains all of your endpoints, parameters, schemas, and authentication schemes.\r\nFern is compatible with the latest OAS release, which is currently \n.\nBelow is an example of an OpenAPI file:\nStart by initializing your Fern folder with an OpenAPI spec\nThis will initialize a directory like the following"
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/troubleshooting.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\ntroubleshooting.mdx\nLatest commit\nHistory\nBreadcrumbs\ntroubleshooting.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://www.githubstatus.com/",
    "text": "Subscribe to our developer newsletter\nProduct\nPlatform\nSupport\nCompany\nResend OTP in: \n seconds \n\n                    Didn't receive the OTP?\n                    \n\n                  \nResend OTP in: \n seconds \n\n                      Didn't receive the OTP?\n                      \n\n                    \nThe URL we should send the webhooks to\nWe'll send you email if your endpoint fails\n\n            For the status of GitHub Enterprise Cloud - EU, please visit: \n\r\nFor the status of GitHub Enterprise Cloud - Australia, please visit: \n\n          \nNo incidents reported today.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nNo incidents reported.\nGet tips, technical guides, and best practices. Twice a month. Right in your\r\n          inbox."
}
,
{
    "url": "https://buildwithfern.com/learn/sdks/introduction/overview",
    "text": "Learn more\nSDK Overview\nLet Fern do the heavy lifting of generating and publishing client libraries so your team can focus on building the API.\nExplore the full list of Fern’s supported languages.\nSee Fern in action with a personalized demo.\nLearn more about advanced features supported by Fern’s SDk generation.\nFollow our step-by-step guide to generate your first SDK in minutes."
}
,
{
    "url": "https://github.com/security",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nTrusted by millions of developers\nSecure platform, secure data\n\nPlatform\nOpen source\nCustomers\nData privacy\nSecurity Highlights\nCloud security and compliance\nReady for best-in-class enterprise security?\nGo further\nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \nWe protect and defend the most trustworthy platform for developers everywhere to create and build software.\nWe’re constantly improving our security, audit, and compliance solutions with you in mind.\nWe keep GitHub safe, secure, and free of spam and abuse so that this can be the platform where developers come together to create.\nWe inspire and enable the community to secure open source at scale, so the world’s software we all depend on sits on foundations you can trust.\nWe help our customers' security and risk teams feel confident in their decisions to encourage developer collaboration on GitHub.\nGitHub is committed to developer privacy and provides a high standard of privacy protection to all our developers and customers. We apply stringent individual privacy protections to all GitHub users worldwide, regardless of their country of origin or location.\nto help protect your data.\nGitHub is a Trusted Cloud Provider(™) with the Cloud Security Alliance (CSA), having completed the self-assessment and third-party assessment for Level 1 CSA STAR Registry and Level 2 STAR Certification. Government users can confidently host projects on GitHub Enterprise Cloud, knowing our platform meets the low impact SaaS security standards set by U.S. federal partners.\nGitHub provides end-to-end DevSecOps, where security is embedded directly into the developer workflow—empowering you to ship secure software fast.\n\n\nBoost your software security with access to GitHub's comprehensive vulnerability database.\n\n\nGitHub Security Lab’s mission is to inspire and enable the community to secure the open source software we all depend on.\nDevelop, scale, and innovate with the assurance of GitHub’s commitment to security, privacy, and compliance.\nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/voice-isolator.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nvoice-isolator.mdx\nLatest commit\nHistory\nBreadcrumbs\nvoice-isolator.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://elevenlabs.io/blog/narrate-any-project",
    "text": "Studio just got bigger & better\nSTUDIO\nExplore more\nOn track to help 1 million people regain their voice\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nFooter\nLong-form text to audio creation for every storyteller\n\nTransforming your narrative into audio has never been easier.  In our latest set of updates to Studio, we’ve redesigned the interface and added new functionalities to help make long-form narration faster and more personalized. \nStudio now features a refined UI/UX, with a new navbar that functions similarly to ‘Google Docs’, allowing quick, intuitive formatting adjustments.\nWe've added a Generation History button where you can restore and download previous generations ensuring flexibility with every take.\nWith our new lock button you can secure any sentence or paragraph once satisfied, preventing accidental changes and helping you track completed sections.\nWith arbitrary regenerations you can select a word or part of a sentence and regenerate only that section, allowing you to fine-tune specific audio outputs, whilst being mindful of character credits.\nWe listened to your feedback and introduced more voice controls and settings, this means you now have more direction over your audio outputs making narrating long-form content quicker than ever. \nYou can:\nWhen adding voices, your text will be highlighted with matching color symbols on the left, providing a visual guide to easily identify different speakers and spot any errors at-a-glance. \nFor an even faster review, we’ve added a keyboard shortcut “Cmd+Opt+A” (Mac) or “Ctrl+Alt+A” (Windows) to see your assigned voices. \nStudio is more than a long-form text-to-audio tool—it’s a creative platform to narrate your story. From scripts to audiobooks, Studio makes it easy to tell your story, at any scale.\nYour comprehensive workflow for turning books into audiobooks and scripts into podcasts\n\n.\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/conversational-ai",
    "text": "Conversational AI\nBuild AI Agents that speak\nLow latency, configurable and scalable\nThe complete Conversational AI toolkit\nThe full developer platform for deploying realistic and extensive voice agents\nLow latency\nCombine our Turbo TTS model with our fine tuned transcription service all on one server.\nAdvanced turn taking\nOur custom interruption detection and turn taking system means agents know when to speak perfect for realtime agents.\nBring any LLM\nSwap between Gemini, Claude, GPT any time or bring your own custom implementation.\nExternal function calling\nIntegrate with any third party app to get real time information or take action.\nThousands of voices\nFind a voice for any use case, situation or character from our library or clone your own.\nAudit and evaluate\nMonitor calls with full transcripts, recordings and automated evaluation.\n31 languages (and counting)\nCreate multilingual agents that can speak with your customers in their native language.\nBuilt on your knowledge base\nImport existing documentation so your agents know everything about your business and products.\nTake phone calls\nIntegrate seamlessly with Twilio using μ-law 8000 Hz audio encoding.\n$0.08 per minute on Business plans, significantly discounted pricing at higher volumes\nUse cases\nCustomer Support\nScheduling\nOutbound Sales\nGaming\nEducation\nFan Engagement\nQuickly build and deploy custom agents for your website or product\nStartups get started free\nPower conversational AI at scale\nFrequently asked questions\nInsights from our team\nHow do you optimize latency for Conversational AI?\nBuilding an effective Voice Agent for our own docs\nThe State of Conversational Voice AI in Education\nThe State of Conversational AI in Support\nTalk to DeepSeek R1 with ElevenLabs Conversational AI\nFooter\nConversational AI\nAdd voice to your agents on web, mobile or telephony in minutes. Our realtime API delivers low latency, full configurability, and seamless scalability.\nGet the best combination of ultra-low latency, high quality voices, along with turn taking and interruption handling that can stand up to unpredictable conversations in the noisiest environments. \n\nConversational AI combines Speech to Text (transcription), an LLM, and Text to Speech, along with logic to handle turn taking and interruption handling to allow for natural back and forth conversation. \nPricing\nGet 15,000 minutes of Conversational AI included in the Business Plan, with extra minutes billed at $0.08/minute.\nHandle a wide range of customer inquiries 24/7, reducing wait times and improving customer satisfaction. Agents can troubleshoot issues, process returns, and even upsell products, all while maintaining a consistent brand voice.\nSimplify appointment booking and management, from healthcare to personal services. Clients can easily schedule, reschedule, or cancel appointments through natural conversation, reducing no-shows and improving operational efficiency.\nConduct personalized cold calling at scale, qualifying leads and setting appointments for your team. Agents can adapt their pitch based on customer responses, ensuring higher engagement and conversion rates.\nCreate immersive gaming experiences, responding dynamically to player actions and choices. Intelligent NPCs (Non-Player Characters) can offer quests, provide guidance, and even adapt their personalities based on the player's behavior, enhancing game depth and replayability.\nProvide personalized learning experiences, offering explanations, quizzes, and adaptive lessons based on a student's progress. Support various subjects and learning styles, making education more accessible and engaging.\nCreate interactive experiences for fans, from trivia games to personalized content recommendations. Provide real-time updates, answer FAQs about players or celebrities, and even simulate conversations with fictional characters, deepening fan connection and loyalty.\nDevelopers\nBuild with our React, Javascript, Python and Swift SDKs.\nGRANTS\n 11M Characters\n 3 months free\n Higher concurrency limits\nENTERPRISE\n✓ Enterprise-level SLAs \n✓ Dedicated support\n✓ Priority access\n✓ API access\n✓ Unlimited seats\n✓ Volume discounts\nLatency is what separates good Conversational AI applications from great ones\nSuccessfully resolving >80% of user inquiries\nHow Conversational Voice AI presents an opportunity to 10x learning outcomes \nConversational AI can make providing support cheaper and more scalable\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://support.github.com/request/landing",
    "text": ""
}
,
{
    "url": "https://github.com/fern-api/fern/blob/main/fern.schema.json",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nfern.schema.json\nLatest commit\nHistory\nBreadcrumbs\nfern.schema.json\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://github.com/fern-api/fern/blob/main/fern-generators.schema.json",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\nfern-generators.schema.json\nLatest commit\nHistory\nBreadcrumbs\nfern-generators.schema.json\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://github.com/fern-api/fern/blob/main/fern/pages/docs/getting-started/docs-overview.mdx?plain=1",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\ndocs-overview.mdx\nLatest commit\nHistory\nBreadcrumbs\ndocs-overview.mdx\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://status.buildwithfern.com/",
    "text": "System status\nDocs\nDocs\nSDKs\nSDKs\nCalendar\n·"
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/commits/main/fern/docs/pages/resources/error-messages.mdx",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nCommits\nBreadcrumbs\nUser selector\nDatepicker\nCommit History\nCommits on Jan 6, 2025\nCommits on Jan 2, 2025\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://raw.githubusercontent.com/elevenlabs/elevenlabs-docs/refs/heads/main/fern/docs/pages/overview.mdx",
    "text": "---\ntitle: ElevenLabs\nheadline: ElevenLabs docs\nsubtitle: ElevenLabs is an AI audio research and deployment company.\nimage: https://elevenlabs.io/cover.png\nhide-nav-links: true\n---\n\n{/* Light mode wave */}\n\n\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://github.com/fern-api/fern/blob/main/generators-yml.schema.json",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nBreadcrumbs\ngenerators-yml.schema.json\nLatest commit\nHistory\nBreadcrumbs\ngenerators-yml.schema.json\nFile metadata and controls\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://github.com/sitemap",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nSitemap\nExplore GitHub\nGitHub Platform\nAdditional Resources\nSupport\nGitHub\nLearn\nDiscover\nDevOps\nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://github.com/git-guides",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nGit Guide\nWhat is Git?\nWhat is Git Written in?\nWhy Use Git?\nSpeed\nMerge conflicts\nCheap branches\nEase of roll back\nHow Do I Use Git?\nLearning Git Basics\nGetting Started With the Git Workflow\nGetting Started With GitHub\nGet started with git and GitHub\nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \nEverything you need to know about Git, from getting started to advanced commands and workflows.\nGit is a distributed version control software. Version control is a way to save changes over time without overwriting previous versions. Being distributed means that every developer working with a Git repository has a copy of that entire repository – every commit, every branch, every file. If you're used to working with centralized version control systems, this is a big difference!\nWhether or not you've worked with version control before, there are a few things you should know before getting started with Git:\nThe tools that make up the core Git distribution are written in C, Shell, Perl, and Tcl. You can find Git's source code on GitHub under \n.\nVersion control is very important – without it, you risk losing your work. With Git, you can make a \"commit\", or a save point, as often as you'd like. You can also go back to previous commits. This takes the pressure off of you while you're working. Commit often and commit early, and you'll never have that gut-sinking feeling of overwriting or losing changes.\nThere are many version control systems out there – but Git has some major advantages.\nLike we mentioned above, Git uses SHA compression, which makes it very fast.\nGit can handle merge conflicts, which means that \n. This opens up the world of development in a way that isn't possible with centralized version control. You have access to the entire project, and if you're working on a branch, you can do whatever you need to and know that your changes are safe.\nSpeaking of branches, Git offers a lot of flexibility and opportunity for collaboration with branches. \nInstead of only committing code that is 100% sure to succeed, developers can commit code that might still need help. Then, they can push that code to the remote and get fast feedback from integrated tests or peer review.\nWithout sharing the code through branches, this would never be possible.\nIf you make a mistake, it's OK! Commits are immutable, meaning they can't be changed. (\n) This means that if you do make a mistake, even on an important branch, like \n, it's \n. \nThe benefits of this can't be overstated. Not only does it create a safer environment for the project and code, but it fosters a development environment where developers can be braver, trusting that Git has their back.\nIf you're getting started with Git, a great place to learn the basic commands is the \n. It's translated into many languages, \n, and a great starting place for the fundamentals on the command line.\nSome of the most important and most used commands that you'll find there are:\nIf you're looking for more GitHub-specific technical guidance, check out \n or our \n on YouTube.\nDepending on your operating system, you may already have \n. But, getting started means more than having the software! To get started, it's important to know the basics of how Git works. You may choose to do the actual work within a terminal, an app like GitHub Desktop, or through GitHub.com. (\n)\nThere are \n ways to use Git, which doesn't necessarily make it easier! But, the fundamental Git workflow has a few main steps. You can practice all of these in the \n.\nThe main branch is usually called \n. We want to work on \n branch, so we can make a pull request and make changes safely. To get started, create a branch off of \n. Name it however you'd like – but we recommend naming branches based on the function or feature that will be the focus of this branch. One person may have several branches, and one branch may have several people collaborate on it – branches are for a purpose, not a person. Wherever you currently \"are\" (wherever HEAD is pointing, or whatever branch you're currently \"checked out\" to) will be the parent of the branch you create. That means you can create branches from other branches, tags, or any commit! But, the most typical workflow is to create a branch from \n – which represents the most current production code.\nOnce you've created a branch, and moved the HEAD pointer to it by \"checking out\" to that branch, you're ready to get to work. Make the changes in your repository using your favorite text editor or IDE.\nNext, save your changes. You're ready to start the commit!\nTo start your \n, you need to let Git know what changes you'd like to include with \n.\nOnce you've saved and staged the changes, you're ready to \n with \n.\nSo far, if you've made a commit locally, you're the only one that can see it. To let others see your work and begin collaboration, you should \"push\" your changes using \n. If you're pushing from a branch for the first time that you've created locally, you may need to give Git some more information. \n tells Git to push the current branch, and create a branch on the remote that matches it with the same name – and also, create a relationship with that branch so that \n will be enough information in the future.\nBy default, \n only pushes the branch that you've currently checked out to.\nSometimes, if there has been a new commit on the branch on the \n, you may be blocked from pushing. Don't worry! Start with a simple \n to incorporate the changes on the remote into your own local branch, resolve any conflicts or finish the merge from the remote into the local branch, and then try the push again.\nPushing a branch, or new commits, to a remote repository is enough if a pull request already exists, but if it's the first time you're pushing that branch, you should open a new pull request. A pull request is a comparison of two branches – typically \n, or the branch that the feature branch was created from, and the feature branch. This way, like branches, pull requests are scoped around a specific function or addition of work, rather than the person making the changes or the amount of time the changes will take.\nPull requests are the powerhouse of GitHub. Integrated tests can automatically run on pull requests, giving you immediate feedback on your code. Peers can give detailed code reviews, letting you know if there are changes to make, or if it's ready to go.\nMake sure you start your pull requests off with the right information. Put yourself in the shoes of your teammates, or even of your future self. Include information about what this change relates to, what prompted it, what is already done, what is left to do, and any specific asks for help or reviews. Include links to relevant work or conversations. Pull request templates can help make this process easy by automating the starting content of the body of pull requests.\nOnce the pull request is open, then the real fun starts. It's important to recognize that pull requests aren't meant to be open when work is \n. Pull requests should be open when work is \n! The earlier you open a pull request, the more visibility the entire team has to the work that you're doing. When you're ready for feedback, you can get it by integrating tests or requesting reviews from teammates.\nIt's very likely that you will want to make more changes to your work. That's great! To do that, make more commits on the same branch. Once the new commits are present on the remote, the pull request will update and show the most recent version of your work.\nOnce you and your team decide that the pull request looks good, you can merge it. By merging, you integrate the feature branch into the other branch (most typically the \n branch). Then, \n will be updated with your changes, and your pull request will be closed. Don't forget to delete your branch! You won't need it anymore. Remember, branches are lightweight and cheap, and you should create a new one when you need it based on the most recent commit on the \n branch.\nIf you choose not to merge the pull request, you can also close pull requests with unmerged changes.\nIf you're wondering where Git ends and GitHub begins, you're not alone. They are tied closely together to make working with them both a seamless experience. While Git takes care of the underlying version control, GitHub is the collaboration platform built on top of it. GitHub is the place for pull requests, comments, reviews, integrated tests, and so much more. Most developers work locally to develop and use GitHub for collaboration. That ranges from using GitHub to host the shared remote repository to working with colleagues and capitalizing on features like protected branches, code review, GitHub Actions, and more.\nThe best place to practice using Git and GitHub is the \n.\nIf you already know Git and need to sign up for a GitHub account, head over to \n.\n\n  \n on GitHub.\n\nReview code, manage projects, and build software alongside 40 million developers.\nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://github.com/about",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\n\n        \n\n        Let's build from here\n      \n\n        The complete developer platform to build, scale, and deliver secure software.\n      \n\n          Blog \n\n\n        \n\n          Brand assets \n\n\n        \n\n          Community stories \n\n\n        \n\n          Customer stories \n\n\n        \n\n          Careers \n\n\n        \n\n          Diversity, Inclusions & Belonging \n\n\n        \n\n          GitHub Status \n\n\n        \n\n          Leadership \n\n\n        \n\n          Octoverse \n\n\n        \n\n          Policy \n\n\n        \n\n          Press \n\n\n        \n\n          Social Impact \n\n\n        \nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \n150\n million\nDevelopers\n4\n million\nOrganizations\n420\n million\nRepositories\n90%\nFortune 100\n\n          Read up on product innovations and updates, company announcements, community spotlights, and more.\n        \n\n          Want to use Mona the octocat? Looking for the right way to display the GitHub logo for your latest project? Download the assets and see how and where to use them.\n        \n\n          Developers are building the future on GitHub every day, explore their stories, celebrate their accomplishments, and find inspiration for your own work.\n        \n\n          See how some of the most influential businesses around the world use GitHub to provide the best services, products, and experiences for their customers.\n        \n\n          Help us build the home for all developers. We’re a passionate group of people dedicated to software development and collaboration. Come join us!\n        \n\n          We are dedicated to building a community and team that reflects the world we live in and pushes the boundaries of software innovation. Learn more about our DI&B efforts.\n        \n\n          We are always monitoring the status of github.com and all its related services. Updates and status interruptions are posted in real-time here.\n        \n\n          Meet the leadership team guiding us as we continue on this journey building the world’s largest and most advanced software development platform in the world.\n        \n\n          Dive into the details with our annual State of the Octoverse report looking at the trends and patterns in the code and communities that build on GitHub. \n        \n\n          We’re focused on fighting for developer rights by shaping the policies that promote their interests and the future of software. Learn more about our policy efforts.\n        \n\n          Explore the latest press stories on our company, products, and global community.\n        \n\n          Learn about how GitHub’s people, products, and platform are creating positive and lasting change around the world.\n        \nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://elevenlabs.io/blog/conversational-voice-ai-in-education",
    "text": "The State of Conversational Voice AI in Education\nUninterrupted focus: how voice enables deeper immersion\nInterruptibility: turning passive consumption into active inquiry\nAssessment: from teaching to insight generation\nWhat these patterns tell us about learning\nFrom insights to the future of learning\nExplore more\nIntroducing Conversational AI\nTIME Brings Conversational AI to Journalism\nFooter\nHow Conversational Voice AI presents an opportunity to 10x learning outcomes \nIn 1988, Benjamin Bloom, an educational psychologist at the University of Chicago, wrote an essay purporting tutorials as \"the best learning conditions we can devise.\" Tutors, Bloom claimed, could raise student achievement by two full standard deviations—or, in statistical parlance, two \"sigmas\"—essentially going from the 50th percentile to the 98th. \nFor forty years, education technology has chased a seemingly impossible goal: matching the effectiveness of one-on-one tutoring at scale. While the potential impact was clear, the economics never worked, until now.\nWhile human tutors typically cost $30-50 per hour, \n solutions can deliver comparable outcomes at $3-5, a 90% lower cost. This order of magnitude cost reduction represents the potential to transform personalized education from a luxury good into reach for the mass-market.\nBut the most profound insights aren't about cost savings—they're about how voice is revealing counterintuitive truths about focus, engagement, and assessment that could reshape education entirely. Early implementations by companies like \n, \n, and \n point to a future where technology doesn't just make learning cheaper, but redefines what effective learning looks like.\nWhen Chess.com \n a voice to their Dr. Wolf instructor, they expected the primary benefit would be accessibility. Instead, they discovered that voice enabled a fundamentally deeper level of focus for learners.\nWith voice guidance, students could keep their eyes locked on the board and their fingers on the pieces, fully immersing themselves in the spatial and strategic dimensions of the game. \"Voice is not just a feature—it's brought a whole new dimension to learning chess online,\" explains Gabe Jacobs, the product manager for Dr. Wolf. \nBy freeing students from the need to constantly shift their visual attention, voice allowed them to stay in a state of uninterrupted concentration. With work underway to add conversational ability to the app, it's a new age for the game of kings.\nThe need for students to split visual focus may be a hidden tax on digital learning—one that voice can eliminate. When students can stay visually focused on the primary learning material, whether chessboard or calculus, they can engage more deeply and grasp concepts more intuitively. \nVoice becomes a way to maintain immersion while still receiving guidance, enabling a kind of ambient support that enhances understanding rather than disrupting it.\nCoursology's experience takes this insight even further, showing how interruption, when implemented intentionally, can transform learning. Coursology, which started as an AI homework helper, grew from zero to 50,000 users in its first month by helping students understand course materials more effectively. With its newest feature, Coursology users can interrupt AI-generated podcasts with questions, turning passive content consumption into active exploration.\n\"The aha moment is when students realize they can upload their most impenetrable course materials and engage in a dialogue with speakers who know everything about those topics,\" explains founder Colby Schmidt. Rather than breaking focus, the ability to spontaneously interrogate the material enables students to learn more deeply and efficiently.\nThis feature reveals a counterintuitive truth about engagement: the ability to interrupt without losing momentum may be more important than delivering flawless, linear content. The natural back-and-forth of questions and answers keeps students engaged with the material itself, not just plodding along a predetermined lesson plan.\nAt one-tenth the cost of human tutoring, AI allows this kind of dynamic interaction to be scaled to millions of students. The implications are profound: with tools like Coursology, the availability of on-demand personalized tutoring will grow by an order of magnitude in the coming years. Coursology itself is about to tip one million users.\nSchoolAI's evolution reveals yet another unexpected opportunity. One feature allows students to learn through conversation: high school history students can speak with figures like Abraham Lincoln or Amelia Earhart, for example. What started as a platform for interactive conversations evolved into something more fundamental: a powerful tool for understanding how learning happens.\nBy capturing data on where students struggle, what helps them break through, and how their understanding develops, SchoolAI gives teachers unprecedented visibility into their students' learning journeys. \"We're not replacing teachers—we're giving them Ironman suits,\" explains CTO Cahlan Sharp. \"We're saving them over 10 hours a week on assessment, so they can focus on actually teaching.\"\nThis points to perhaps the most exciting potential of voice AI in education: the ability to surface actionable insights about the learning process itself. Every voice interaction becomes a data point, illuminating patterns and interventions that would be impossible to discern at scale otherwise. In this sense, the real value of AI may lie not in the teaching, but in the metaknowledge it generates about teaching.\nThese early implementations don't just suggest incremental improvements—they reveal fundamental truths about how people learn. Each discovery challenges our basic assumptions about education.\nFirst, visual attention is more precious than we imagined. Traditional digital learning often forces students to context-switch between reading instructions, watching demonstrations, and practicing skills. This isn't just inconvenient—it's cognitively expensive. Voice liberates visual attention, allowing students to focus entirely on understanding what's in front of them.\nSecond, the ability to interrupt without losing context might be more important than perfect instruction. The natural flow of questions and answers maintains engagement better than even the most polished linear content. At one-tenth the cost of human tutoring, companies can now scale this kind of dynamic interaction to millions of students.\nThird, the real value of AI in education might not be in the teaching itself, but in what it reveals about how learning happens. Every interaction creates data about where students struggle, what helps them break through, and how understanding develops. SchoolAI's evolution shows how this insight alone can transform education—giving teachers unprecedented visibility into their students' learning journeys.\nFor builders exploring conversational AI, these early signals suggest opportunities to not just optimize existing educational practices, but to reimagine them entirely:\nThe shift that voice represents in education may be as profound as the transition from scheduled broadcast to on-demand streaming in entertainment. Just as platforms like Netflix enabled fundamentally new kinds of storytelling, conversational AI could enable entirely new modes of learning. \"The shift in education over the next decade will be from memorizing things to actually retaining knowledge,\" notes Schmidt. This isn't just speculation—it's already happening.\nWhen Chess.com lets students maintain complete visual focus on the board, when Coursology enables interruption without disruption, when SchoolAI gives teachers unprecedented insight into how their students learn—these aren't just features. They're early signals of how voice AI could reshape education entirely.\nIn our own data, we’ve seen EdTech lead the charge in conversational AI usage since February 2024. Alongside sales and support, EdTech use cases continue to appear, complementing existing features and inspiring new ones.\nFor product teams, this represents more than just a feature opportunity. It's a chance to fundamentally rethink how technology can enhance learning. The economics tell one story: AI tutoring at one-tenth the cost means personalized learning could finally reach everyone. But the more profound opportunity lies in what these early implementations reveal: that conversational AI might be the key to unlocking deeper understanding. \nThe companies that will define this era will be those that see voice not just as a medium for delivering content, but as an unprecedented window into how we learn—insights that will shape the lives and livelihoods of generations to come.\nOur all in one platform for building customizable, interactive voice agents\nBuild a deeper understanding through 1:1 conversations\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://raw.githubusercontent.com/elevenlabs/elevenlabs-docs/refs/heads/main/fern/docs/pages/resources/error-messages.mdx",
    "text": "---\ntitle: Error messages\nsubtitle: Explore error messages and solutions.\n---\n\nThis guide includes an overview of error messages you might see in the ElevenLabs dashboard & API.\n\n## Dashboard errors\n\n| Error Message                                          | Cause                                                                                                     | Solution                                                                                                                                                        |\n| ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| The selected model can not be used for text-to-speech. | Occurs when switching between speech-to-speech and text-to-speech if the model does not switch correctly. | Select the desired model. If unresolved, select a different model, then switch back.                                                                            |\n| Oops, something went wrong.                            | Indicates a client-side error, often due to device or browser issues.                                     | Click “Try again” or refresh the page. If unresolved, clear browser cache and cookies. Temporarily pause browser-based translation tools like Google Translate. |\n\n\n\n\n## API errors\n\n### Code 400/401\n\n| Code                               | Overview                                                                                                                                                                                                   |\n| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| max_character_limit_exceeded \n | **Cause:** You are sending too many characters in a single request. \n **Solution:** Split the request into smaller chunks, see [character limits](/docs/models#character-limits) for more information. |\n| invalid_api_key                    | **Cause:** You have not set your API key correctly. \n **Solution:** Ensure the request is correctly authenticated. See [authentication](/docs/api-reference/authentication) for more information.      |\n| quota_exceeded                     | **Cause:** You have insufficient quota to complete the request. \n **Solution:** On the Creator plan and above, you can enable usage-based billing from your Subscription page.                         |\n| voice_not_found                    | **Cause:** You have entered the incorrect voice_id. \n **Solution:** Check that you are using the correct voice_id for the voice you want to use. You can verify this in My Voices.                     |\n\n### Code 403\n\n| Code              | Overview                                                                                                                                                                   |\n| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| only_for_creator+ | **Cause:** You are trying to use professional voices on a free or basic subscription. \n **Solution:** Upgrade to Creator tier or higher to access professional voices. |\n\n### Code 429\n\n| Code                               | Overview                                                                                                                                                                                                |\n| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| too_many_concurrent_requests \n | **Cause:** You have exceeded the concurrency limit for your subscription. \n **Solution:** See [concurrency limits and priority](/docs/models#concurrency-limits-and-priority) for more information. |\n| system_busy                        | **Cause:** Our services are experiencing high levels of traffic and your request could not be processed. \n **Solution:** Retry the request. If the issue persists, please contact support.          |"
}
,
{
    "url": "https://raw.githubusercontent.com/elevenlabs/elevenlabs-docs/refs/heads/main/fern/docs/pages/resources/troubleshooting.mdx",
    "text": "---\ntitle: Troubleshooting\nsubtitle: Explore common issues and solutions.\n---\n\nOur models are non-deterministic, meaning outputs can vary based on inputs. While we strive to enhance predictability, some variability is inherent. This guide outlines common issues and preventive measures.\n\n## General\n\n\n\n\n## Studio (formerly Projects)"
}
,
{
    "url": "https://www.instagram.com/github/",
    "text": ""
}
,
{
    "url": "https://elevenlabs.io/blog/auto-regenerate-is-live-in-projects",
    "text": "Auto-regenerate is live in Studio\nSTUDIO\nExplore more\nStudio just got bigger & better\nOn track to help 1 million people regain their voice\nFooter\nOur long form text editor now lets you regenerate faulty fragments, adjust playback speed, and provide quality feedback\nEvery time you generate audio, we check for mispronunciations and unwanted artifacts. If we find any issues, the system automatically regenerates the audio at no extra cost.\nYou can enable this feature in two places:\nWe’ve also introduced a \nwhich lets you flag sub-par audio generations. This feedback helps us refine our internal error detection and improve future results.\nThese updates, along with the ability to playback audio at 2x speed, have reduced the time to needed to quality-check long form content by up to 70% among our beta group.\nYour comprehensive workflow for turning books into audiobooks and scripts into podcasts\n\n.\nLong-form text to audio creation for every storyteller\n\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/startup-grants",
    "text": "ElevenLabs Grants\nBuild human-like voices into your new product or startup with an ElevenLabs Grant\nEverything you need to get started\n3 months free\n11,000,000 characters\nScale\nHow to apply\nSubmit an application\nNotification of acceptance\nStart building\nBuild human-like voices into your new product or startup\nElevenLabs Grants FAQ \nFooter\nElevenLabs Grants\nto build, test, and launch your product with ElevenLabs voices and sounds.\nincluded per month. That's over 200 hours of generated audio.\nlevel benefits including high scale capacity and early access to new features.\nFill out the application using the button above. In our short application, tell us more about the product you're building, your team, how you expect to grow.\nWe'll let you know if your application is accepted within one week.\nOnce your grant is awarded, we'll apply the grant plan and credits to the account you applied with!\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://github.com/about/developer-policy",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nFighting for developers\nEvery developer has the right to do their best work. Period.\nInnovation\nCollaboration\nEqual Opportunity\n\n      How we make an impact\n    \nProtecting open source from EU copyright law\nAdvancing developer freedom and access in Iran\nEstablishing the Developer Defense Fund\n\n    \n\n  \nPlatform responsibility\nInnovation\nOpen source\nGlobal collaboration\nInclusion\nGet involved\nOpen an issue\nSend us an email\nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \nAs the home for all developers, GitHub is committed to shaping policies that promote their interests and the future of software. We work on policy in its many forms—laws, regulations, norms, and standard practices—to fight for developers when it matters most.\nNo matter who they are or where they are, we believe developers should be able to work on the projects they care about. This means advocating for policies that promote developers’ most basic rights: The rights to innovation, collaboration, and equal opportunity.\nAhmad Awais/\n“This I believe is a big win for open source. Thank you GitHub!”\nCarlos Bordachar/\n“Bien hecho! 👏”\nAndrew Eiche/\n“Let this be the new gold standard of fighting for your users!”\nAndrew Eiche/\n“Let this be the new gold standard of fighting for your users!”\nJeff Mosawy/\n“It couldn't get any better! A big THANK YOU to GitHub&nbsp ❤️ ”\nAhmad Awais/\n“This I believe is a big win for open source. Thank you GitHub!”\nSamin Aref/\n“Thank you Nat Friedman and GitHub for 👏 advocating for disadvantaged developers 👏 standing up against discriminatory rules and 👏 fighting for your values.”\n“Great work by the team at GitHub 👏 This is amazing news for both Iranian developers and global developer collaboration.”\nJeff Mosawy/\n“It couldn't get any better! A big THANK YOU to GitHub&nbsp ❤️ ”\nDevelopers constantly solve new problems and build on previous solutions. Being a developer requires the freedom to experiment, learn, and share.\nSoftware development is a global exercise in teamwork. In order to innovate, build, and keep the world’s software secure, developers must be able to work together.\nSoftware development thrives in an open and inclusive environment. When everyone has equal access and opportunity to contribute, we all benefit.\nWhen the EU began working on the Copyright Directive in 2016, software developers weren’t on policymakers’ minds. Drafts effectively required upload filters that would have wreaked havoc on online software collaboration. We worked to mobilize developers and convey their concerns to members of parliament, and together we succeeded: The final text included an exemption for “open source software developing and sharing platforms.”\nNo matter where developers may live, we’re committed to keeping as much of GitHub available to as many developers as possible. As part of this commitment to support software development everywhere, we make the case to governments for licenses and changes to the law. Starting in 2019, we worked with the US government’s Office of Foreign Assets Control to obtain a license to fully serve developers in Iran. Building on this success, we’re pursuing similar efforts to be able to fully serve developers in Crimea and Syria, and are also advocating for changes to the law so that all developers can have access — whether they use GitHub or other collaboration tools. \nSection 1201 of the US DMCA can pose challenges to developers. Each year, GitHub receives a small number of 1201 takedown claims, contending that developers are hosting code that illegally circumvents a technological protection measure. To help, GitHub has committed $1 million to establish a Developer Defense Fund and sponsored the GitHub Developer Rights Fellowship at Stanford Law School’s Juelsgaard Intellectual Property and Innovation Clinic.\nProtecting developers’ ability to collaborate by providing a safe and inclusive space that transparently respects rights to free expression, assembly, and association.\nAdvocating for copyright, patent, and employment rules that foster exploration, collaboration, and the freedom to tinker on side projects.\nEncouraging governments to use open source as a tool of public policy, to work proactively with open source contributors, and to support open source software in legislation and regulation.\nSupporting developers to work with anyone, anywhere, so they can build secure and useful software that benefits their local communities.\nBreaking down barriers so anyone can become a developer and we can build a developer community that reflects our global diversity.\nGitHub is leading the way for developers’ rights worldwide. Our Policy team uses its voice to advocate for the developer community and shape policies that impact the software ecosystem as a whole.\nNeed help on a developer policy or want to share feedback?\nInterested in collaborations or scheduling a meeting?\nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://elevenlabs.io/blog/building-an-agent-for-our-own-docs",
    "text": "Building an effective Voice Agent for our own docs\nStrengths\nLimitations\nAgent Configuration: \nExplore more\nIntroducing Conversational AI\nMeet Flash\nFooter\nSuccessfully resolving >80% of user inquiries\nAt ElevenLabs, we recently embedded a Conversational AI agent in our docs to help reduce the support burden for documentation-related questions (Test it out \n). Our support agent is now successfully handling \n of user inquiries across \n. These results demonstrate the potential for AI to augment traditional documentation support while highlighting the continued importance of human support for complex queries. In this post, I will detail our iterative process you can follow to replicate our success. \nWe set out to build an agent that can:\nWe implemented two layers of evaluation:\n: For each call, our built-in evaluation tooling runs through the finished conversation and evaluates if the agent has been successful. The criteria is fully customizable. We ask if the agent solved the user inquiry, or was able to redirect them to a relevant support channel. \nWe have been able to steadily improve the ability of the LLM to solve or redirect the inquiry successfully, reaching 80% according to our evaluation tooling. \nNow, it’s important to consider that not all types of support queries or questions can be solved by an LLM, especially for a startup that builds fast and innovates constantly, and with extremely technical and creative users. As an additional disclaimer, an evaluation LLM will not evaluate correctly 100% of the time.\n: To contrast the efficacy of our LLM validation tooling, we conducted a human validation of 150 conversations, using the same evaluation criteria provided to the LLM tooling:\nThe human evaluation also revealed that \nof relevant support questions were answered or redirected correctly by the Documentation agent.\nOther findings: \nThe LLM-powered agent is adept at resolving clear and specific questions that can be answered with our documentation, pointing callers to the relevant documentation, and providing some initial guidance on more complex queries. In most of these cases, the agent provides quick, straightforward, and correct answers that are immediately helpful. \nQuestions include: \nRecommendations:\nOn the flip side, the agent is less helpful with account issues, pricing/discount questions, or non-specific questions that would benefit from deeper investigation / querying. Also, issues that are fairly vague and generic -> despite being prompted to ask questions, the LLM usually favours answering with something that might seem relevant from the documentation. \nQuestions include: \nRecommendations\nAlongside the prompt, we are passing the LLM a Knowledge Base of relevant information in the context. This knowledge base includes a summarised, but still large (80k characters) version of all ElevenLabs documentation, as well as some relevant URLs. \nWe are also adding clarifications and FAQs as part of the knowledge base. \nWe have three tools configured:\nOur evaluation tooling involves an LLM going over the final transcript and assessing the conversation against defined criteria. \nOur documentation agent has proven to be effective for helping users navigate common product and support questions, and is an engaging copilot for users navigating our docs. We’re able to consistently iterate and improve our agent through continuous automated and manual monitoring. We recognize that not all types of support queries or questions can be solved by an LLM, especially for a startup that builds fast and innovates constantly, and with extremely technical and creative users. But we’ve found that the more we are able to automate, the more time our team can spend focused on tackling the tricky and interesting problems that come up at the margins as our community continues to push the boundaries of what is possible with AI Audio. \nOur agent is powered by \n. If you’d like to reproduce my results, you can \n and follow my steps. If you get stuck, you can speak to the agent we’ve deployed \n or get in touch with me and my team in \n. For high volume use cases (>100 calls per day), \n.\nOur all in one platform for building customizable, interactive voice agents\nYou’ve never experienced human-like TTS this fast\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://github.com/fern-api/fern/commits/main/fern.schema.json",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nCommits\nBreadcrumbs\nUser selector\nDatepicker\nCommit History\nCommits on Feb 19, 2025\nCommits on Feb 18, 2025\nCommits on Feb 17, 2025\nCommits on Feb 6, 2025\nCommits on Jan 30, 2025\nCommits on Dec 23, 2024\nCommits on Nov 27, 2024\nCommits on Nov 21, 2024\nCommits on Nov 19, 2024\nCommits on Nov 4, 2024\nCommits on Oct 21, 2024\nCommits on Sep 29, 2024\nCommits on Sep 19, 2024\nCommits on Aug 12, 2024\nCommits on Aug 10, 2024\nCommits on Aug 9, 2024\nCommits on Aug 6, 2024\nCommits on Jul 24, 2024\nCommits on Jul 16, 2024\nCommits on Jun 18, 2024\nCommits on Jun 13, 2024\nCommits on Jun 5, 2024\nCommits on May 20, 2024\nCommits on Apr 23, 2024\nCommits on Apr 18, 2024\nCommits on Apr 2, 2024\nCommits on Feb 28, 2024\nCommits on Feb 20, 2024\nCommits on Feb 6, 2024\nCommits on Jan 31, 2024\nCommits on Nov 21, 2023\nCommits on Nov 20, 2023\nCommits on Aug 21, 2023\nPagination\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n."
}
,
{
    "url": "https://github.com/about/leadership",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\n\n    Our leadership\n  \n\n        Thomas Dohmke\n      \n\n              Thomas Dohmke\n            \n\n        Kyle Daigle\n      \n\n              Kyle Daigle\n            \n\n        Shelley McKinley\n      \n\n              Shelley McKinley\n            \n\n        Elizabeth Pemmerl\n      \n\n              Elizabeth Pemmerl\n            \n\n        Mario Rodriguez\n      \n\n              Mario Rodriguez\n            \n\n        Justin Thenutai\n      \n\n              Justin Thenutai\n            \n\n        Alexis Wales\n      \n\n              Alexis Wales\n            \n\n        Chris Shi\n      \n\n              Chris Shi\n            \nCareers\nDiversity & Inclusion\nSite-wide Links\nSubscribe to our developer newsletter\n\n          Product\n        \n\n          Platform\n        \n\n          Support\n        \n\n          Company\n        \nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \nMeet the leaders behind the world’s leading software development platform.\n\n              Chief Executive Officer\n            \nThomas Dohmke is CEO of GitHub and drives the company’s mission of making GitHub the home for all developers. Fascinated by software development since his childhood, Thomas is passionate about building tools developers love and creating products that drive software development forward. He previously co-founded HockeyApp and led the company as CEO through its acquisition by Microsoft in 2014. Thomas holds a PhD in mechanical engineering from University of Glasgow, UK. He lives in the Seattle area, is a LEGO enthusiast, and enjoys coding and making his GitHub contribution graph green in his spare time.\n\n              Chief Operating Officer\n            \nKyle is Chief Operating Officer at GitHub, leading teams responsible for culture, developer outreach, operations, and communications. Joining GitHub in 2013, Kyle built and scaled GitHub's ecosystem engineering teams and worked on the acquisitions of Semmle, pm, and others. Eleven years (and many ships) later, Kyle is just as committed to driving growth across the business and its people, leading GitHub's own Al adoption strategy across a workforce of 3,000 + talented Hubbers. As a developer himself, Kyle is passionate about bringing software practices to operations and works to preserve and grow the spirit of GitHub as an Al-integrated, developer-first company.\nPrior to GitHub, Kyle took on developer-focused challenges as an engineering and product leader in startups, working in FinTech, real estate, and consulting. When he isn't collaborating with Hubbers and customers, he's building home automations with Home Assistant, working with nonprofits to make technology available and accessible to all, gaming (hello Xbox), and traveling with family.\n\n              Chief Legal Officer\n            \nShelley is the Chief Legal Officer at GitHub, where she leads teams responsible for Trust and Safety, Social Impact, Developer Policy, Product & Regulatory Legal, Commercial Legal, and Legal Operations. Shelley started her career at Microsoft supporting parts of the Developer Division in 2005. Prior to joining GitHub, she was the head of Microsoft’s Technology and Corporate Responsibility organization, where she oversaw a team that drove the use of technology to benefit society through priorities such as Accessibility, Environmental Sustainability, Broadband Access, Responsible AI, and Justice Reform. She has also led legal, corporate, and external affairs teams across Europe and worked on products in Microsoft’s gaming division, including developer communities and Xbox Live. Prior to joining Microsoft, she was legal counsel at Wizards of the Coast, where she supported the “Dungeons & Dragons” and “Magic: The Gathering” brands.\nShelley is an outspoken proponent of mental health awareness and uses her voice to help address the affiliated bias and stigma, by advocating for a culture of openness and inclusion. Outside of work, you can find her enjoying outdoor concerts, tearing up the slopes on her snowboard and relaxing with her family and friends.\n\n              Chief Revenue Officer\n            \nElizabeth is Chief Revenue Officer at GitHub. As CRO, she is responsible for all aspects of our go-to-market and customer experience, including sales, customer success, support and operations. She is focused on GitHub’s revenue growth, strategic planning, operational excellence, and expanding our successful motions with parent company Microsoft.\nElizabeth joined GitHub in 2015 to build our US public sector business, a market where she had significant prior experience. Her tenure at GitHub has included leadership roles in nearly all aspects of our enterprise business. \nWhen she isn’t talking to customers about GitHub, she enjoys playing tennis and running. She can frequently be found on dog walks with her daughter and husband in their metro DC neighborhood. \n\n\n\n              Chief Product Officer\n            \nMario Rodriguez is GitHub's Chief Product Officer. He leads the GitHub Product team, which includes Product, Design, and Product Operations. His core identity is being a learner and his passion is creating developer tools—so much so that he has spent the last 20 years living that mission in leadership roles across Microsoft and GitHub. In addition to the product portfolio, Mario oversees GitHub’s AI/Copilot strategy, having previously launched and grown Copilot across thousands of organizations and millions of users.\nOutside of GitHub, Mario likes to spend time outside with his wife and two daughters. He also co-chairs and founded a charter school in an effort to progress education in rural regions of the United States.\n\n\n\n              Chief People Officer\n            \nJustin Thenutai is the Chief Human Resources Officer for GitHub and joined the team in March 2024. He is responsible for establishing and driving GitHub’s people priorities in partnership with the company’s Executive Leadership Team and the HR organization. As a member of the GitHub Leadership Team, he plays a critical role in shaping the employee experience and cultivating a culture of inclusion. \nPrior to this role, Justin led HR teams at Microsoft for a broad portfolio of Engineering organizations, including Windows, Surface, Mixed Reality, Device Operations, Teams, OneDrive, SharePoint, Mesh, Data Platform + Growth, and Viva.  Before serving as an HR General Manager, Justin led strategy, planning and operations for Global Talent Acquisition and drove the centralization of company recruiting efforts. He also led recruiting teams for Marketing and Business Development, Search/Advertising and Datacenters, and Tech Incubations and Microsoft Research. Before Microsoft, Justin led recruiting at an executive search firm. \nJustin is based in the Seattle area where he lives with his wife and two kids. Outside of work and family, Justin is involved with several non-profit organizations, including serving on the Board of Directors for the Global Mentorship Initiative (GMI). His work with GMI provides Justin with the opportunity to partner with underserved college students around the world to realize their full potential as they prepare to start their professional careers. He is a native of Southern California and attended UC Irvine where he studied Psychology & Social Behavior and Criminology.\n\n              Chief Information Security Officer\n            \nAlexis Wales is the Chief Information Security Officer of GitHub. She leads a team of security experts focused on safeguarding the GitHub platform, products and the open source community, empowering more than 150 million developers worldwide to build and deploy software securely on GitHub.\nAlexis has 20 years of experience defending critical national and private sector networks, spanning positions with the Department of Defense and the Department of Homeland Security’s Cybersecurity and Infrastructure Security Agency (CISA). This experience sparked her passion for collaboration between the public and private sectors to solve the hardest security challenges that threaten the technology we use every day.\nAlexis is a self-described \"borderless\" leader. She believes that, just as threats and vulnerabilities transcend borders, the most significant security impact is achieved through open dialogue and collaboration to make risk-informed decisions and achieve secure outcomes. Outside of work, Alexis enjoys staying active through yoga and cherishes her “long” walks to the mailbox with her senior pug, Lola.\n\n              Chief Financial Officer at GitHub\n            \nChris Shi is the Chief Financial Officer at GitHub. Since joining the team, Chris has brought her extensive experience in Cloud and AI products from her time at Microsoft. With a strong background in the developer space, she’s excited to continue driving innovation in finance and supporting GitHub’s mission. Prior to Microsoft, Chris worked in consulting and earned an M.B.A from Northwestern University’s Kellogg School of Management.\nOutside of work, Chris enjoys traveling, reading, trying out new restaurants, and exploring different parts of Seattle through “urban hiking.”\nChris is looking forward to continuing to get to know the GitHub team and contributing to the company’s ongoing growth and success.\n\n        Help us build the home for all developers. We’re a passionate group of people dedicated to software development and collaboration. Come join us!\n      \n\n        We work hard to build the most inclusive communities we can—online, on our teams, and in cities around the world.\n      \nGet tips, technical guides, and best practices. Twice a month. Right in your inbox."
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-docs/tree/50404a5aadadb22fec9bf41497307383f104c9b7",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nelevenlabs/elevenlabs-docs\nFolders and files\nLatest commit\nHistory\nRepository files navigation\nElevenLabs Documentation\nRunning the docs\nOther developer resources\nSDKs\nAbout\nTopics\nResources\nStars\nWatchers\nForks\n\n  \n\n  \n\n  \nLanguages\nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \n\n        Documentation for elevenlabs.io/docs\n      \nOur documentation is hosted at \n and all of the content lives inside the \n folder.\nInstall the \n to preview the documentation changes locally. To install, use the following command\nRun the following command at the root of the folder\nWe support the following SDKs which can be used to interact with the ElevenLabs platform:\n\n        Documentation for elevenlabs.io/docs"
}
,
{
    "url": "https://elevenlabs.io/blog/talk-to-deepseek-r1-with-elevenlabs-conversational-ai",
    "text": "Talk to DeepSeek R1 with ElevenLabs Conversational AI\nGet started with ElevenLabs Conversational AI\nSet up DeepSeek R1 with Cloudflare Workers AI\nConfigure your custom LLM in ElevenLabs\nTest it out\nConclusion\nExplore more\nBest use cases for conversational AI agents\nBuilding conversational AI applications with advanced text to speech APIs\nFooter\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nEveryone has been talking about DeepSeek lately, but no one has stopped to ask: what would DeepSeek sound like if it could talk? This is where we can utilize \n to make it possible to talk to DeepSeek R1.\nElevenLabs Conversational AI is a platform for deploying customized, realtime conversational voice agents. One major benefit of the platform is its flexibility, allowing you to plug in different LLMs depending on your needs. \nThe \n option works with any OpenAI-compatible \nprovider, as long as the model supports tool use/function calling. In our docs you can find guides for \n, \n, and \n. For the following demo we’re using Cloudflare.\nWe’re using the DeepSeek-R1-Distill-Qwen-32B model which is a model distilled from DeepSeek-R1 based on Qwen2.5. It outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. \nThe reason we’re using the distilled version is mainly that the pure versions \n with function calling. In fact the R1 reasoning model doesn’t yet support function calling at all! You can follow \n to stay updated on their progress.\nThe process of connecting a custom LLM is fairly straightforward and starts with creating an agent. For this project we’ll use the Math Tutor template. If you just want to see what DeepSeek sounds like, we've got a demo of the agent you can \n. \nTo get started head to the \n and create a new AI Agent called “\n”. Select the Math tutor template as this is a use case where DeepSeek’s reasoning can shine, and select “\n”.\nThe math template includes a system prompt that ensures it reads out numbers and equations in natural language, rather than as written by the LLM. For example we tell it to write \"\n\" if the response from the LLM is \"\n\". \nReasoning models will generally explain their “\n” but we can also expand the system prompt to make extra sure we get a good math lecture such as specifying that it provides reasoning step-by-step for the solution. You can leave this general or expand the prompt to include specific examples it can draw from.\nHowever, if you really want to get a sense of DeepSeek R1’s personality you can instead create a blank template asking the agent to introduce itself as DeepSeek AI and setting the system prompt to tell it to answer a range of questions on different topics. \nNext up we need to get an endpoint for a version of DeepSeek R1 to use with our agent. In this example we’ll use Cloudflare which provides DeepSeek-R1-Distill-Qwen-32B on their \n for evaluation purposes, but you can also consider other hosting options like \n, \n, and  \nFirst, head over to \n and create or sign in to your account. In the navigation, select AI > Workers AI, and then click on the “Use REST API” widget.\nTo get an \n key, click the “Create a Workers AI API Token”, and make sure to store it securely.\nOnce you have your API key, you can try it out immediately with a curl request. Cloudflare provides an OpenAI-compatible API endpoint making this very convenient. At this point make a note of the model and the full endpoint — including the account ID.\nFor example: \nNext step is to add these details to your agent in ElevenLabs. First, navigate to your DeepSeeker AI Agent in the \n and at the very bottom, add your newly created \nkey as a secret. Give it a key such as CLOUDFLARE_AUTH_TOKEN, then save the changes. \nNext, under LLM, select the Custom LLM option, and provide Cloudflare Workers AI OpenAI-compatible\n endpoint: https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/v1 \nMake sure to replace “{ACCOUNT_ID}” in the URL with your account ID shown in Cloudflare.\nAdd the model ID: @cf/deepseek-ai/deepseek-r1-distill-qwen-32b\nFinally select your secret that you created just now. \nGreat, so now that everything is set up, let’s click “Test AI agent” and \n with the following math problem:\n“Two trains pass each other going east/west with rates of 65mph and the other 85mph. How long until they are 330 miles apart?”\nBuilding realtime conversational AI agents with the ElevenLabs Conversational AI platform allows you to quickly react to new models being released by allowing you to plugin custom LLMs through any OpenAI-compatible API provider.\nAs long as the model you want to use is OpenAI compatible and supports function calling you can integrate it with a realtime voice agent. This could include models you’ve fine-tuned on custom datasets. You can also use any voice available from ElevenLabs library — or your own voice if you’ve \n.\nEnhance conversational AI applications with natural dialogue.\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/blog/the-state-of-conversational-ai-in-support",
    "text": "The State of Conversational AI in Support\nThe support status quo\nConversational AI in action\nThe future\nExplore more\nBest use cases for conversational AI agents\nBuilding your first conversational AI agent: A beginner’s guide\nFooter\nConversational AI can make providing support cheaper and more scalable\nStripe is synonymous with great support, often going the extra mile to surprise and delight. However, great support is more than just the handwritten notes and 3D-printed totems we see on Twitter. More than a decade’s worth of human and software engineering has enabled Stripe to build a customer service offering that supports millions of businesses a year. I joined Stripe in 2015, when the company had just tipped 200 people, and the support team fit around one conference table.\nI was one of the first hires in Dublin, an early outpost in Stripe’s global support coverage. In my five years there, Stripe re-organized the support team into specializations, introduced external vendors as Tier 1 support, expanded channels, added languages, and, eventually, paid support plans. Having helped design and test these evolving support programs, I saw firsthand the complexities of scaling customer service—complexities that conversational AI could have streamlined.\nConversational AI has three elements. These three elements work together seamlessly: Speech-to-Text captures the user’s intent, Language Models interpret and generate intelligent responses, and Text-to-Speech transforms those responses into natural conversation.\nWhen I joined Stripe, my job was to ingest as much of the product as I could. As part of my training, I would shadow more experienced agents, and learn from a litany of prior support interactions—a similar approach to how an AI agent would learn.\nAI is great at pattern recognition, and much of support is a pattern recognition exercise. It starts with a user explaining their issue. The support agent processes that information, tries to match that to their existing knowledge, and presents an answer back to the user. However, support is more than just the recitation of a knowledge base; great customer service requires empathy, creativity, and working together with a user to solve their issue. Unfortunately, most support interactions are found lacking.\nSupport is an important touchpoint, but it’s often painful. Think of the last time you had to call an airline. You’re on hold for hours and — when you finally get through to someone — you’re told that your issue relates to another department, and the Sisyphean cycle continues.\nThe pain isn’t just on the customer side. Providing a great support experience is hard, and even harder at scale. In our software-centric world of bits, support is largely constrained by atoms. Imagine you get a large influx of customers overnight: you can just spin up an AWS instance to keep your site up; you can’t just flick a switch and turn on support.\nIt takes time and money to scale up a support function. In North America, finding, training, and onboarding a support agent costs about $12,000. Including that initial outlay, the average cost for a support agent is about $30-$40 per hour. Some companies choose to outsource their support agents, with an hourly rate ranging from $8-30 per hour. Appealing on the surface, but with that low price comes a lack of control over accountability, processes, and support quality.\nBut what if you could spin up support in minutes, and scale it infinitely? What if you could have high-quality support, at a low cost? That’s the promise of\n, and having spent years working in support, I believe it’s a total game-changer.\nAlexis is one example of an AI agent from ElevenLabs. It was built as a support agent \n and it now handles over one hundred calls a day. \nImagine Alexis was human. To start, I don’t believe many human support agents could handle 100 calls a day. If you had a very efficient agent and straightforward calls, they could do about 60, and probably closer to 40 for a less-skilled agent or more complex queries. So you’d need to hire at least two agents, and probably pay some overtime.\nIf the agents were based in North America, the cost for them to handle 100 calls would be at least $700, about $1719 on average, and it could be up to $4094. That’s a lot, so you might decide to outsource. That would be somewhere between $128 and $480, averaging $288 — a lot cheaper on the face of it, but there are hidden costs in onboarding and overseeing these outsourcing companies, as well as brand and reputational risk if they aren’t up to your standards.\nThe cost of running an AI Support Agent can be as low as $0.015 per minute for the audio component. The LLM can be less than a penny to a couple of cents per minute, depending on your model choice and the size of your knowledge base. So, that would be about $13-18 for one hundred calls. That’s over 110x cheaper than the average internal agent, and 18x cheaper than the average outsourced one. As an objective support experience, I think Alexis is pretty great. At that price point, he’s incredible. \nIn the largely analog world of supply chain, \n is a technological bridge for talent, helping industrial businesses recruit and manage their workers. While Traba provides apps and dashboards, phone communication remains a critical connector for a workforce used to paper timesheets and cork job boards. \nTraba looked at what phone calls its operations team spent the most time on and found two dominant areas: fielding support queries or running through scheduling checklists. For CTO Akshay Buddiga, automation was the obvious answer, but timing is also important. “When we identify a process we want to make more efficient, we prefer to embed automation early rather than trying to retrofit solutions later.”\nTraba is laser-focused on unit economics and staying ahead of the curve on tech innovation, so moving these calls to conversational AI was appealing for multiple reasons. With speed and cost in mind, the team went looking at solutions. But user experience was also key. As software engineer Joseph Besgen noted: “We wanted it to sound like a conversation, not just listening to a recording.” The ElevenLabs demo sounded so real that, during testing, a Traba employee’s father couldn’t tell whether it was AI or someone from his son’s team. \nWhile there’s an inherent complexity to scheduling thousands of light industry workers, it shouldn’t be as hard to schedule one (1) doctor’s appointment. But have you tried calling your doctor lately? It’s frustrating for patients, and an inefficient use of healthcare staff’s time. In partnership with ElevenLabs,\n assistants are filling the gap in healthcare administration, running everything from appointment scheduling to billing. One hospital now has 86% of its calls handled entirely by AI agents. This results in not only a 66% reduction in cost per call, but also efficiency gains for the administrative staff. EliseAI has also increased access to healthcare for non-English speaking communities. \nSome interesting trends emerge when we consider conversational AI growth by sector. EdTech was the first industry we saw embrace conversational AI – as companies could finally provide customized one-on-one tutoring and language learning at an affordable price. Customer support applications quickly followed, as support interactions are particularly well-suited because they are pattern-recognition exercises: the answer exists in a knowledge base, and the AI agent works to match that to the user’s question. We also see a rise in verticalized end-to-end applications, particularly in sectors like logistics and healthcare – Traba and EliseAI, for example. Again, these are areas that are repetitive and predictable, so easily handled by an AI agent.\n brings support from the atomic realm into the world of bits. Now, support can be a better experience for companies and their customers. Like your cloud provider, your AI support team can scale up and down. Your customers won't ever have to wait on hold, and you save yourself the operational headache of spinning up support centers across the globe.\nFor the past year, voice agents have been good at conversation and knowledge retrieval, but I think that’s just the start. In 2025, I believe \n will be the standard protocol for inbound meeting scheduling and “product specialist” type support issues. Although they may be limited to knowledge-retrieval calls, this will make a big dent on support volume and free up human teams to do higher-value work.\nBy 2026, we will have moved from retrieving knowledge to performing actions. The standard AI voice agent will make API calls and connect with third-party apps. They will routinely perform actions like book meetings and issue refunds.\nIn 2027, I see AI voice agents progressing from support to customer success. I believe things like entire sales deals will be run by an AI agent, possibly on both the buyer and seller side. For many, this seems like the final frontier, but I think it's just the beginning. Deeply contextual and creative tasks, which we once believed were exclusively human, will increasingly become the domain of AI.\nSupport will be transformed from a cost-center: first neutralizing expenses, and eventually becoming a profit-center for the business. AI voice agents will proactively reach out to customers, reducing churn and increasing customer LTV. Conversational AI provides many of the benefits of human support, but it also comes with perfect recall, tens of languages, and is operational 24/7. The future is coming, and in many ways, it's already here. Imagine instant, empathic, effective support – every single time. Soon, you’ll almost want to call an airline, just for the fun of it.\nA simple guide to creating a hyper-realistic conversational AI agent.\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://github.com/elevenlabs/elevenlabs-examples",
    "text": "Navigation Menu\nSearch code, repositories, users, issues, pull requests...\n\n        Provide feedback\n      \n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\nLicense\nelevenlabs/elevenlabs-examples\nFolders and files\nLatest commit\nHistory\nRepository files navigation\nElevenLabs Examples\n🚀 Featured Projects\nConversational AI Demos\nText-to-Speech (TTS) Demos\nNative Mac App (Open Source)\nSound Effects Generation\nAudioNative React Demo\nDubbing API Demo\nPronunciation Dictionaries\n🛠 Getting Started\n🤝 Contributing\n📚 Learn More\n📄 License\nAbout\nTopics\nResources\nLicense\nStars\nWatchers\nForks\n\n  \n\n  \n\n  \nFooter\nFooter navigation\nWe read every piece of feedback, and take your input very seriously.\n\n            To see all available qualifiers, see our \n.\n          \n\n  \nThis collection of demos and projects showcases the ElevenLabs API and how you can start building next generation AI audio apps with it. Whether you're looking to integrate text-to-speech into your website, create dubbed content, or explore advanced conversational applications, you'll find valuable resources here.\nThese projects offer practical examples of building real-time, voice-driven applications with rich interactivity.\nA fully open-source native Mac application that brings ElevenLabs to your desktop. Written by Claude 3.5 and Cursor.\nUnleash your creativity with our sound effects generation demo. Create custom audio landscapes for your projects!\nEmbed ElevenLabs' text-to-speech capabilities directly into your React-based websites. This demo shows you how to seamlessly integrate our technology for a native-like audio experience.\nDiscover how to use our Dubbing API to create multilingual content effortlessly. Perfect for content creators and localization teams!\nLearn how to work with pronunciation dictionaries to fine-tune the output of our voice models.\nTo get started with these examples:\nFor detailed API documentation and guides, visit our \n.\nWe welcome contributions from the community! Before you start:\nThis project is licensed under the MIT License - see the \n file for details."
}

],
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/blog",
    "text": "Blog\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nOn track to help 1 million people regain their voice\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nWe’re partnering with Star Sports to localize cricket content across languages\nHow text to speech enhances virtual tours and immersive experiences\nAnnouncing the ElevenLabs Award Winners in the Project Odyssey AI Film Competition\nRian scales global storytelling with ElevenLabs’ AI Voice technology\nThe state of AI Audio in publishing and news\nAiMation Studios uses ElevenLabs to bring characters to life\nAI Dubbing for Polish Presidency of the Council of the EU\nElevenLabs joins the EU AI Champions Initiative\nWe cut our pricing for Conversational AI\nSolda.AI launches AI voice agents that can close deals\nGemini 2.0 Flash now available\nConvin adds AI voice calling to its contact center platform\nStudio is now available to everyone\nProduction-Ready Conversational AI at Enterprise Scale: With Scale AI’s Felix Su\nArianna Huffington uses ElevenLabs to refresh Thrive for its 10th anniversary\nHow Conversational AI will change entertainment and media\nTalk to DeepSeek R1 with ElevenLabs Conversational AI\nElevenLabs raises $180M Series C to be the voice of the digital world\nFooter\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nGiving cricket fans across India access to their favorite player's insights in English, Hindi, Tamil and other Indian languages.\nBring virtual experiences to life with compelling text to speech narration.\nAfter reviewing 4500 submissions (190 hours of film!) to the world's largest AI Film Festival – Project Odyssey - we can now finally reveal the grand prize winners of the ElevenLabs award.\nRian accelerates global storytelling with ElevenLabs’ AI dubbing — boosting efficiency, authenticity, and audience reach.\nAI audio is transforming the news industry, boosting engagement and accessibility through narrated articles and podcasts.\nAiMation Studios uses ElevenLabs Voice AI and sound effects models to bring characters to life and speed up its pipeline.\nPress conferences will be available in Polish, English, and French using AI-generated voices\nOver 60 companies are working to strengthen Europe’s role in global AI\nCalls now start at 10 cents per minute — an ~50% discount across Starter, Creator and Pro plans\nGemini 2.0 Flash is now available to all developers building with ElevenLabs Conversational AI\nUsing our speech technology for AI voice agents\nStudio, our longform text-to-audio editor for creators and storytellers, is now available to everyone.\nBuilding controlled AI experiences through smart architecture and guardrails\nArianna Huffington turns to ElevenLabs Voice AI technology to refresh the preface of her book Thrive for its 10th anniversary. \nConversational AI is reshaping entertainment and media, enabling more interactive and personalized experiences\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nAnd make speech the new standard for digital interaction\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/app/sign-up",
    "text": ""
}
,
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/docs/api-reference/text-to-speech/convert",
    "text": "Create speech\nPath parameters\nQuery parameters\nRequest\nResponse\nErrors\nConvert text to speech using our library of over 3,000 voices across 32 languages.\nID of the voice to be used. Use the \n endpoint list all the available voices.\nWhen enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.\nYou can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:\n0 - default mode (no latency optimizations)\n1 - normal latency optimizations (about 50% of possible latency improvement of option 3)\n2 - strong latency optimizations (about 75% of possible latency improvement of option 3)\n3 - max latency optimizations\n4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).\nDefaults to None.\nThe output format of the generated audio.\nThe text that will get converted into speech.\nIdentifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.\nLanguage code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\nVoice settings overriding stored setttings for the given voice. They are applied only on the given request.\nA list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.\nThe text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nThe text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nA list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.\nA list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.\nIf true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.\nThis parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.\nSuccessful Response"
}

],
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/blog",
    "text": "Blog\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nOn track to help 1 million people regain their voice\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nWe’re partnering with Star Sports to localize cricket content across languages\nHow text to speech enhances virtual tours and immersive experiences\nAnnouncing the ElevenLabs Award Winners in the Project Odyssey AI Film Competition\nRian scales global storytelling with ElevenLabs’ AI Voice technology\nThe state of AI Audio in publishing and news\nAiMation Studios uses ElevenLabs to bring characters to life\nAI Dubbing for Polish Presidency of the Council of the EU\nElevenLabs joins the EU AI Champions Initiative\nWe cut our pricing for Conversational AI\nSolda.AI launches AI voice agents that can close deals\nGemini 2.0 Flash now available\nConvin adds AI voice calling to its contact center platform\nStudio is now available to everyone\nProduction-Ready Conversational AI at Enterprise Scale: With Scale AI’s Felix Su\nArianna Huffington uses ElevenLabs to refresh Thrive for its 10th anniversary\nHow Conversational AI will change entertainment and media\nTalk to DeepSeek R1 with ElevenLabs Conversational AI\nElevenLabs raises $180M Series C to be the voice of the digital world\nFooter\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nGiving cricket fans across India access to their favorite player's insights in English, Hindi, Tamil and other Indian languages.\nBring virtual experiences to life with compelling text to speech narration.\nAfter reviewing 4500 submissions (190 hours of film!) to the world's largest AI Film Festival – Project Odyssey - we can now finally reveal the grand prize winners of the ElevenLabs award.\nRian accelerates global storytelling with ElevenLabs’ AI dubbing — boosting efficiency, authenticity, and audience reach.\nAI audio is transforming the news industry, boosting engagement and accessibility through narrated articles and podcasts.\nAiMation Studios uses ElevenLabs Voice AI and sound effects models to bring characters to life and speed up its pipeline.\nPress conferences will be available in Polish, English, and French using AI-generated voices\nOver 60 companies are working to strengthen Europe’s role in global AI\nCalls now start at 10 cents per minute — an ~50% discount across Starter, Creator and Pro plans\nGemini 2.0 Flash is now available to all developers building with ElevenLabs Conversational AI\nUsing our speech technology for AI voice agents\nStudio, our longform text-to-audio editor for creators and storytellers, is now available to everyone.\nBuilding controlled AI experiences through smart architecture and guardrails\nArianna Huffington turns to ElevenLabs Voice AI technology to refresh the preface of her book Thrive for its 10th anniversary. \nConversational AI is reshaping entertainment and media, enabling more interactive and personalized experiences\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nAnd make speech the new standard for digital interaction\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/app/sign-up",
    "text": ""
}
,
{
    "url": "https://elevenlabs.io/docs/api-reference/text-to-speech/convert",
    "text": "Create speech\nPath parameters\nQuery parameters\nRequest\nResponse\nErrors\nConvert text to speech using our library of over 3,000 voices across 32 languages.\nID of the voice to be used. Use the \n endpoint list all the available voices.\nWhen enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.\nYou can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:\n0 - default mode (no latency optimizations)\n1 - normal latency optimizations (about 50% of possible latency improvement of option 3)\n2 - strong latency optimizations (about 75% of possible latency improvement of option 3)\n3 - max latency optimizations\n4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).\nDefaults to None.\nThe output format of the generated audio.\nThe text that will get converted into speech.\nIdentifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.\nLanguage code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\nVoice settings overriding stored setttings for the given voice. They are applied only on the given request.\nA list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.\nThe text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nThe text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nA list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.\nA list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.\nIf true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.\nThis parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.\nSuccessful Response"
}
,
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}

],
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/blog",
    "text": "Blog\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nOn track to help 1 million people regain their voice\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nWe’re partnering with Star Sports to localize cricket content across languages\nHow text to speech enhances virtual tours and immersive experiences\nAnnouncing the ElevenLabs Award Winners in the Project Odyssey AI Film Competition\nRian scales global storytelling with ElevenLabs’ AI Voice technology\nThe state of AI Audio in publishing and news\nAiMation Studios uses ElevenLabs to bring characters to life\nAI Dubbing for Polish Presidency of the Council of the EU\nElevenLabs joins the EU AI Champions Initiative\nWe cut our pricing for Conversational AI\nSolda.AI launches AI voice agents that can close deals\nGemini 2.0 Flash now available\nConvin adds AI voice calling to its contact center platform\nStudio is now available to everyone\nProduction-Ready Conversational AI at Enterprise Scale: With Scale AI’s Felix Su\nArianna Huffington uses ElevenLabs to refresh Thrive for its 10th anniversary\nHow Conversational AI will change entertainment and media\nTalk to DeepSeek R1 with ElevenLabs Conversational AI\nElevenLabs raises $180M Series C to be the voice of the digital world\nFooter\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nGiving cricket fans across India access to their favorite player's insights in English, Hindi, Tamil and other Indian languages.\nBring virtual experiences to life with compelling text to speech narration.\nAfter reviewing 4500 submissions (190 hours of film!) to the world's largest AI Film Festival – Project Odyssey - we can now finally reveal the grand prize winners of the ElevenLabs award.\nRian accelerates global storytelling with ElevenLabs’ AI dubbing — boosting efficiency, authenticity, and audience reach.\nAI audio is transforming the news industry, boosting engagement and accessibility through narrated articles and podcasts.\nAiMation Studios uses ElevenLabs Voice AI and sound effects models to bring characters to life and speed up its pipeline.\nPress conferences will be available in Polish, English, and French using AI-generated voices\nOver 60 companies are working to strengthen Europe’s role in global AI\nCalls now start at 10 cents per minute — an ~50% discount across Starter, Creator and Pro plans\nGemini 2.0 Flash is now available to all developers building with ElevenLabs Conversational AI\nUsing our speech technology for AI voice agents\nStudio, our longform text-to-audio editor for creators and storytellers, is now available to everyone.\nBuilding controlled AI experiences through smart architecture and guardrails\nArianna Huffington turns to ElevenLabs Voice AI technology to refresh the preface of her book Thrive for its 10th anniversary. \nConversational AI is reshaping entertainment and media, enabling more interactive and personalized experiences\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nAnd make speech the new standard for digital interaction\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/app/sign-up",
    "text": ""
}
,
{
    "url": "https://elevenlabs.io/docs/api-reference/text-to-speech/convert",
    "text": "Create speech\nPath parameters\nQuery parameters\nRequest\nResponse\nErrors\nConvert text to speech using our library of over 3,000 voices across 32 languages.\nID of the voice to be used. Use the \n endpoint list all the available voices.\nWhen enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.\nYou can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:\n0 - default mode (no latency optimizations)\n1 - normal latency optimizations (about 50% of possible latency improvement of option 3)\n2 - strong latency optimizations (about 75% of possible latency improvement of option 3)\n3 - max latency optimizations\n4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).\nDefaults to None.\nThe output format of the generated audio.\nThe text that will get converted into speech.\nIdentifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.\nLanguage code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\nVoice settings overriding stored setttings for the given voice. They are applied only on the given request.\nA list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.\nThe text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nThe text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nA list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.\nA list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.\nIf true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.\nThis parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.\nSuccessful Response"
}
,
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}

],
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/blog",
    "text": "Blog\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nOn track to help 1 million people regain their voice\nSpotify is now accepting Audiobooks narrated by ElevenLabs\nWe’re partnering with Star Sports to localize cricket content across languages\nHow text to speech enhances virtual tours and immersive experiences\nAnnouncing the ElevenLabs Award Winners in the Project Odyssey AI Film Competition\nRian scales global storytelling with ElevenLabs’ AI Voice technology\nThe state of AI Audio in publishing and news\nAiMation Studios uses ElevenLabs to bring characters to life\nAI Dubbing for Polish Presidency of the Council of the EU\nElevenLabs joins the EU AI Champions Initiative\nWe cut our pricing for Conversational AI\nSolda.AI launches AI voice agents that can close deals\nGemini 2.0 Flash now available\nConvin adds AI voice calling to its contact center platform\nStudio is now available to everyone\nProduction-Ready Conversational AI at Enterprise Scale: With Scale AI’s Felix Su\nArianna Huffington uses ElevenLabs to refresh Thrive for its 10th anniversary\nHow Conversational AI will change entertainment and media\nTalk to DeepSeek R1 with ElevenLabs Conversational AI\nElevenLabs raises $180M Series C to be the voice of the digital world\nFooter\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nExpanding beyond ALS to support MSA and mouth cancer patients with free AI voice technology\nMore people are listening to audiobooks than ever before. But for many independent authors, getting an audio version of their work published on major platforms has been expensive and time-consuming — until now.\nGiving cricket fans across India access to their favorite player's insights in English, Hindi, Tamil and other Indian languages.\nBring virtual experiences to life with compelling text to speech narration.\nAfter reviewing 4500 submissions (190 hours of film!) to the world's largest AI Film Festival – Project Odyssey - we can now finally reveal the grand prize winners of the ElevenLabs award.\nRian accelerates global storytelling with ElevenLabs’ AI dubbing — boosting efficiency, authenticity, and audience reach.\nAI audio is transforming the news industry, boosting engagement and accessibility through narrated articles and podcasts.\nAiMation Studios uses ElevenLabs Voice AI and sound effects models to bring characters to life and speed up its pipeline.\nPress conferences will be available in Polish, English, and French using AI-generated voices\nOver 60 companies are working to strengthen Europe’s role in global AI\nCalls now start at 10 cents per minute — an ~50% discount across Starter, Creator and Pro plans\nGemini 2.0 Flash is now available to all developers building with ElevenLabs Conversational AI\nUsing our speech technology for AI voice agents\nStudio, our longform text-to-audio editor for creators and storytellers, is now available to everyone.\nBuilding controlled AI experiences through smart architecture and guardrails\nArianna Huffington turns to ElevenLabs Voice AI technology to refresh the preface of her book Thrive for its 10th anniversary. \nConversational AI is reshaping entertainment and media, enabling more interactive and personalized experiences\nWe gave DeepSeek R1 a voice with ElevenLabs Conversational AI platform\nAnd make speech the new standard for digital interaction\nCreate with the highest quality AI Audio\nResearch\nProducts\nSolutions\nEarn As\nResources\nCompany\nReach everyone with AI audio\n© 2025 ElevenLabs"
}
,
{
    "url": "https://elevenlabs.io/app/sign-up",
    "text": ""
}
,
{
    "url": "https://elevenlabs.io/docs/overview",
    "text": "Most popular\nMeet the models\nCapabilities\nProduct guides\nElevenLabs\nElevenLabs is an AI audio research and deployment company.\nLearn how to integrate ElevenLabs\nDeploy voice agents in minutes\nLearn how to use ElevenLabs\nDive into our API reference\nOur most lifelike, emotionally rich speech synthesis model\nOur fast, affordable speech synthesis model\nConvert text into lifelike speech\nModify and transform voices\nIsolate voices from background noise\nDub audio and videos seamlessly\nCreate cinematic sound effects\nClone and design custom voices\nDeploy intelligent voice agents\nExplore our product guides for step-by-step guidance"
}
,
{
    "url": "https://elevenlabs.io/docs/api-reference/text-to-speech/convert",
    "text": "Create speech\nPath parameters\nQuery parameters\nRequest\nResponse\nErrors\nConvert text to speech using our library of over 3,000 voices across 32 languages.\nID of the voice to be used. Use the \n endpoint list all the available voices.\nWhen enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.\nYou can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:\n0 - default mode (no latency optimizations)\n1 - normal latency optimizations (about 50% of possible latency improvement of option 3)\n2 - strong latency optimizations (about 75% of possible latency improvement of option 3)\n3 - max latency optimizations\n4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).\nDefaults to None.\nThe output format of the generated audio.\nThe text that will get converted into speech.\nIdentifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.\nLanguage code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\nVoice settings overriding stored setttings for the given voice. They are applied only on the given request.\nA list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.\nThe text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nThe text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.\nA list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.\nA list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.\nIf true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.\nThis parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.\nSuccessful Response"
}

]